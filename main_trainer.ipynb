{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.decomposition import KernelPCA, TruncatedSVD\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from catboost import CatBoostClassifier\n",
    "from genetic_algorithm import GeneticAlgorithmCV\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from skl2onnx import convert_sklearn, update_registered_converter\n",
    "from skl2onnx.common.shape_calculator import calculate_linear_classifier_output_shapes\n",
    "from skl2onnx.common.data_types import FloatTensorType, Int64TensorType\n",
    "from skl2onnx._parse import _apply_zipmap, _get_sklearn_operator_name\n",
    "from onnx.helper import get_attribute_value\n",
    "from catboost.utils import convert_to_onnx_object\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesamiento de archivos (modificado para devolver 4 componentes)\n",
    "def process_files(file_paths):\n",
    "    processed_data = []\n",
    "    for file_path in file_paths:\n",
    "        model_type = extract_action(file_path)  # 'long' o 'short'\n",
    "        df = pd.read_csv(file_path)\n",
    "        # (1) Generar datos base\n",
    "        X_train, y_train = create_training_dataset(df, model_type)\n",
    "        # (2) Aplicar meta-labeling (devuelve 2 conjuntos)\n",
    "        (X_main, y_main), (X_meta, y_meta) = meta_label_data_multi_bootstrap_oob(\n",
    "            X_train, y_train,\n",
    "            models_number=100,\n",
    "            bad_samples_fraction=0.8\n",
    "        )\n",
    "        processed_data.append({\n",
    "            'direction': model_type,\n",
    "            'main': (X_main, y_main),\n",
    "            'meta': (X_meta, y_meta)\n",
    "        })\n",
    "    return processed_data\n",
    "\n",
    "# Helper function\n",
    "def extract_action(filepath):\n",
    "    match = re.search(r'(long|short)', filepath)\n",
    "    return match.group(0) if match else None\n",
    "\n",
    "# Crear datasets de entrenamiento\n",
    "def create_training_dataset(df, action_type):\n",
    "    print(f\"=== Procesando dataset de {'compras' if action_type == 'long' else 'ventas'} ===\")\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"Total de operaciones después de eliminar duplicados: {len(df)}\")\n",
    "    # Filtrar las operaciones con profit != 0\n",
    "    df_trade = df[df['profit'] != 0].copy()\n",
    "    print(f\"Operaciones con profit != 0: {len(df_trade)}\")\n",
    "    # Añadir la columna 'target' basada en el profit\n",
    "    df_trade['target'] = df_trade['profit'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    num_ganadoras = df_trade['target'].sum()\n",
    "    num_perdedoras = len(df_trade) - num_ganadoras\n",
    "    print(f\"Operaciones ganadoras: {int(num_ganadoras)}\")\n",
    "    print(f\"Operaciones perdedoras: {int(num_perdedoras)}\")\n",
    "    # Eliminar posibles missings\n",
    "    if df_trade.isna().values.any():\n",
    "        num_missings = df_trade.isna().sum().sum()\n",
    "        print(f\"Valores faltantes encontrados: {num_missings}\")\n",
    "        df_trade = df_trade.dropna()\n",
    "        print(f\"Total de operaciones después de eliminar missings: {len(df_trade)}\")\n",
    "    df_training = df_trade.copy(deep=True)\n",
    "    # Seleccionar las columnas necesarias (todas menos las dos últimas para el conjunto principal)\n",
    "    feature_columns = df_training.columns[:-2]\n",
    "    df_training = df_training[feature_columns.tolist() + ['target']]\n",
    "    # Preparación de los datos de entrenamiento\n",
    "    X_train = df_training.drop(columns='target').values.astype('float')\n",
    "    y_train = df_training['target'].values.astype('int')\n",
    "    print(f\"Dataset final preparado: {X_train.shape[0]} operaciones, {X_train.shape[1]} características\")\n",
    "    return X_train, y_train\n",
    "\n",
    "def sample_random_hparams():\n",
    "    \"\"\"\n",
    "    Retorna un dict con hiperparámetros aleatorios de XGBoost \n",
    "    dentro de rangos razonables. Ajusta a tu gusto.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'iterations':       random.randint(50, 500),\n",
    "        'max_depth':        random.randint(3, 10),\n",
    "        'learning_rate':    random.uniform(0.1, 0.5),\n",
    "        'l2_leaf_reg':      random.uniform(0.0, 1.0),\n",
    "        'min_data_in_leaf': random.randint(1, 10)\n",
    "    }\n",
    "\n",
    "def meta_label_data_multi_bootstrap_oob(\n",
    "    X, y, \n",
    "    models_number=5, \n",
    "    bad_samples_fraction=0.8\n",
    "):\n",
    "    \"\"\"\n",
    "    Versión 'avanzada' de meta-labeling con bootstrapping y uso de OOB.\n",
    "    - Cada iteración:\n",
    "      (a) Sample (frac=0.5, replace=True) => train_sample\n",
    "      (b) OOB = filas que quedaron fuera => val_sample\n",
    "      (c) Entrena en train_sample\n",
    "      (d) Predice solo en val_sample\n",
    "      (e) Acumula las filas de val_sample mal clasificadas\n",
    "    - Al final, elimina filas que superan el umbral (mean*bad_samples_fraction)\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(X)\n",
    "    df['target'] = y\n",
    "    # Índices de malas muestras\n",
    "    BAD_0 = pd.Index([])\n",
    "    BAD_1 = pd.Index([])\n",
    "    for i in range(models_number):\n",
    "        # (1) Generar muestra bootstrap (train_sample)\n",
    "        train_sample = df.sample(frac=0.8, replace=True, random_state=None)\n",
    "        # (2) Conjunto OOB = val_sample\n",
    "        val_sample = df.loc[~df.index.isin(train_sample.index)]\n",
    "\n",
    "        # (3) Hiperparámetros (aleatorios o fijos)\n",
    "        hparams = sample_random_hparams()\n",
    "        model = CatBoostClassifier(\n",
    "            task_type=\"CPU\",\n",
    "            eval_metric='Accuracy',\n",
    "            verbose=False,\n",
    "            **hparams\n",
    "        )\n",
    "\n",
    "        # (4) Entrenar en la parte bootstrap\n",
    "        model.fit(\n",
    "            train_sample.drop(columns='target'),\n",
    "            train_sample['target']\n",
    "        )\n",
    "\n",
    "        # (5) Predecir en OOB\n",
    "        if len(val_sample) == 0:\n",
    "            # Si en algún caso random la muestra bootstrap coge todo, esto evitará un error\n",
    "            continue\n",
    "\n",
    "        pred_proba = model.predict_proba(val_sample.drop(columns='target'))[:, 1]\n",
    "        pred_labels = (pred_proba >= 0.5).astype(int)\n",
    "\n",
    "        # (6) Identificar malas muestras solo en val_sample\n",
    "        val_sample = val_sample.copy()\n",
    "        val_sample['pred'] = pred_labels\n",
    "        \n",
    "        val_sample_0 = val_sample[val_sample['target'] == 0]\n",
    "        val_sample_1 = val_sample[val_sample['target'] == 1]\n",
    "        \n",
    "        diff_0 = val_sample_0.index[val_sample_0['target'] != val_sample_0['pred']]\n",
    "        diff_1 = val_sample_1.index[val_sample_1['target'] != val_sample_1['pred']]\n",
    "\n",
    "        # (7) Acumular esos índices\n",
    "        BAD_0 = BAD_0.append(diff_0)\n",
    "        BAD_1 = BAD_1.append(diff_1)\n",
    "    # (8) Contar la frecuencia de error de cada índice\n",
    "    to_mark_0 = BAD_0.value_counts()\n",
    "    to_mark_1 = BAD_1.value_counts()\n",
    "    # (9) Definir umbral\n",
    "    threshold_0 = to_mark_0.mean() * bad_samples_fraction if len(to_mark_0) else 0\n",
    "    threshold_1 = to_mark_1.mean() * bad_samples_fraction if len(to_mark_1) else 0\n",
    "    marked_0 = to_mark_0[to_mark_0 > threshold_0].index if len(to_mark_0) else []\n",
    "    marked_1 = to_mark_1[to_mark_1 > threshold_1].index if len(to_mark_1) else []\n",
    "    # (10) Filtrar las filas marcadas del dataset completo\n",
    "    all_bad = pd.Index(marked_0).union(marked_1)\n",
    "    good_mask = ~df.index.isin(all_bad)\n",
    "    # Crear columna meta_labels\n",
    "    df['meta_labels'] = 1\n",
    "    df.loc[all_bad, 'meta_labels'] = 0\n",
    "    # Devolver X_main (filtrado) y X_meta (todo el dataset)\n",
    "    X_main = df.loc[good_mask].drop(columns=['target', 'meta_labels'])\n",
    "    y_main = df.loc[good_mask, 'target']\n",
    "    X_meta = df.drop(columns=['target', 'meta_labels']) \n",
    "    y_meta = df['meta_labels']\n",
    "    \n",
    "    return (X_main, y_main), (X_meta, y_meta)\n",
    "\n",
    "# Train model functions\n",
    "def train_classifier(X_train, y_train, model_type):\n",
    "    n_features = X_train.shape[1]\n",
    "    # Definir algoritmo de validación cruzada\n",
    "    cv = StratifiedKFold(n_splits=3)\n",
    "    # Definir Pipeline\n",
    "    pipeline = Pipeline([\n",
    "            ('scaler', 'passthrough'),\n",
    "            ('reducer', 'passthrough'),\n",
    "            ('catboostclassifier', CatBoostClassifier(\n",
    "                task_type=\"CPU\",\n",
    "                eval_metric='Accuracy',\n",
    "                verbose=False\n",
    "        ))\n",
    "        ])\n",
    "    # Definir mapa de estimadores\n",
    "    estimator_map = {\n",
    "        'scaler': {\n",
    "            'standard': StandardScaler(),\n",
    "            'robust': RobustScaler(),\n",
    "            'none': 'passthrough'\n",
    "        },\n",
    "        'reducer': {\n",
    "            'kernel_pca_rbf': KernelPCA(kernel='rbf'),\n",
    "            'kernel_pca_linear': KernelPCA(kernel='linear'),\n",
    "            'truncated_svd': TruncatedSVD(),\n",
    "            'none': 'passthrough'\n",
    "        }\n",
    "    }\n",
    "    # Definir espacio de hiperparámetros para compras\n",
    "    param_grid = {\n",
    "        'scaler': {'type': 'categorical', 'values': ['standard', 'robust']},\n",
    "        'reducer': {'type': 'categorical', 'values': [\n",
    "            'kernel_pca_rbf', 'kernel_pca_linear', 'truncated_svd']},\n",
    "        'reducer__n_components': {'type': 'int', 'low': 2, 'high': n_features-1},\n",
    "        'catboostclassifier__iterations': {'type': 'int', 'low': 50, 'high': 500},\n",
    "        'catboostclassifier__max_depth': {'type': 'int', 'low': 3, 'high': 10},\n",
    "        'catboostclassifier__learning_rate': {'type': 'float', 'low': 0.1, 'high': 0.5},\n",
    "        'catboostclassifier__l2_leaf_reg': {'type': 'float', 'low': 0.0, 'high': 1.0},\n",
    "        'catboostclassifier__min_data_in_leaf': {'type': 'int', 'low': 1, 'high': 10}\n",
    "    }\n",
    "    try:\n",
    "        # Entrenar el modelo utilizando el algoritmo genético\n",
    "        ga_search = GeneticAlgorithmCV(\n",
    "            model_type=model_type,\n",
    "            pipeline=pipeline,\n",
    "            param_grid=param_grid,\n",
    "            estimator_map=estimator_map,\n",
    "            cv=cv,\n",
    "            pop_size=25,\n",
    "            generations=5,\n",
    "            early_stopping_rounds=3,\n",
    "            crossover_initial=0.9,\n",
    "            crossover_end=0.1,\n",
    "            mutation_initial=0.1,\n",
    "            mutation_end=0.9,\n",
    "            elitism=True,\n",
    "            elite_size=5,\n",
    "            tournament_size=3,\n",
    "            n_random=5,\n",
    "            n_jobs=1,\n",
    "            verbose=True,\n",
    "        )\n",
    "        ga_search.fit(X_train, y_train)\n",
    "    except Exception as e:\n",
    "        print(f\"Error en traing model {model_type}: {e}\")\n",
    "        raise\n",
    "    # Obtener los mejores parámetros y el mejor estimador\n",
    "    print(\"####################################################################\")\n",
    "    print(f\"Mejor puntuación de validación para {model_type}: {ga_search.best_score_}\")\n",
    "    print(f\"Mejores parámetros encontrados para {model_type}: {ga_search.best_params_full_}\")\n",
    "    print(\"####################################################################\")\n",
    "    # Retornar mejor estimador\n",
    "    return ga_search.best_estimator_\n",
    "\n",
    "# ONNX para Pipeline con Catboost\n",
    "def skl2onnx_parser_castboost_classifier(scope, model, inputs, custom_parsers=None):\n",
    "    options = scope.get_options(model, dict(zipmap=True))\n",
    "    no_zipmap = isinstance(options[\"zipmap\"], bool) and not options[\"zipmap\"]\n",
    "    \n",
    "    alias = _get_sklearn_operator_name(type(model))\n",
    "    this_operator = scope.declare_local_operator(alias, model)\n",
    "    this_operator.inputs = inputs\n",
    "    \n",
    "    label_variable = scope.declare_local_variable(\"label\", Int64TensorType())\n",
    "    probability_tensor_variable = scope.declare_local_variable(\"probabilities\", FloatTensorType())\n",
    "    \n",
    "    this_operator.outputs.append(label_variable)\n",
    "    this_operator.outputs.append(probability_tensor_variable)\n",
    "    \n",
    "    return _apply_zipmap(options[\"zipmap\"], scope, model, inputs[0].type, this_operator.outputs)\n",
    "\n",
    "def skl2onnx_convert_catboost(scope, operator, container):\n",
    "    onx = convert_to_onnx_object(operator.raw_operator)\n",
    "    node = onx.graph.node[0]\n",
    "    \n",
    "    container.add_node(\n",
    "        node.op_type,\n",
    "        [operator.inputs[0].full_name],\n",
    "        [operator.outputs[0].full_name, operator.outputs[1].full_name],\n",
    "        op_domain=node.domain,\n",
    "        **{att.name: get_attribute_value(att) for att in node.attribute}\n",
    "    )\n",
    "\n",
    "def save_onnx_model(mql5_files_folder, model, X, model_type):\n",
    "    try:\n",
    "        # Define el tipo de entrada\n",
    "        initial_type = [('input', FloatTensorType([None, X.shape[1]]))]\n",
    "        \n",
    "        # Convierte el pipeline completo\n",
    "        model_onnx = convert_sklearn(\n",
    "            model,\n",
    "            initial_types=initial_type,\n",
    "            target_opset={\"\": 12, \"ai.onnx.ml\": 2},\n",
    "            options={id(model.steps[-1][1]): {'zipmap': True}}\n",
    "        )\n",
    "        \n",
    "        # Guarda el modelo\n",
    "        with open(os.path.join(mql5_files_folder, f\"model_{model_type}.onnx\"), \"wb\") as f:\n",
    "            f.write(model_onnx.SerializeToString())\n",
    "            \n",
    "        print(f\"Modelo {model_type} ONNX exportado correctamente\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error en exportar el modelo {model_type}: {e}\")\n",
    "        raise\n",
    "\n",
    "def train_models_parallel(data_list, mql5_files_folder):\n",
    "    # Antes de cualquier entrenamiento o conversión:\n",
    "    update_registered_converter(\n",
    "        CatBoostClassifier,\n",
    "        \"CatBoostClassifier\",\n",
    "        calculate_linear_classifier_output_shapes,\n",
    "        skl2onnx_convert_catboost,\n",
    "        parser=skl2onnx_parser_castboost_classifier,\n",
    "        options={\"nocl\": [True, False], \"zipmap\": [True, False, \"columns\"]}\n",
    "    )\n",
    "    # Diccionario para rastrear metadatos de cada futuro\n",
    "    future_metadata = {}\n",
    "    futures = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        # Paso 1: Enviar todas las tareas de entrenamiento\n",
    "        for data in data_list:\n",
    "            direction = data['direction']\n",
    "            \n",
    "            # Modelo Principal (main)\n",
    "            X_main, y_main = data['main']\n",
    "            future_main = executor.submit(\n",
    "                train_classifier, \n",
    "                X_main, \n",
    "                y_main, \n",
    "                f\"{direction}_main\"  # Ej: \"long_main\"\n",
    "            )\n",
    "            future_metadata[future_main] = {\n",
    "                'type': 'main',\n",
    "                'direction': direction,\n",
    "                'X_train': X_main\n",
    "            }\n",
    "            futures.append(future_main)\n",
    "            \n",
    "            # Meta-Modelo (validación causal)\n",
    "            X_meta, y_meta = data['meta']\n",
    "            future_meta = executor.submit(\n",
    "                train_classifier,\n",
    "                X_meta,\n",
    "                y_meta.astype(int), \n",
    "                f\"{direction}_meta\"  # Ej: \"long_meta\"\n",
    "            )\n",
    "            future_metadata[future_meta] = {\n",
    "                'type': 'meta',\n",
    "                'direction': direction,\n",
    "                'X_train': X_meta\n",
    "            }\n",
    "            futures.append(future_meta)\n",
    "        \n",
    "        # Paso 2: Procesar resultados conforme se completan\n",
    "        for future in as_completed(futures):\n",
    "            metadata = future_metadata[future]\n",
    "            try:\n",
    "                model = future.result()\n",
    "                model_type = f\"{metadata['direction']}_{metadata['type']}\"  # Ej: \"long_main\"\n",
    "                X_train = metadata['X_train']\n",
    "                # Guardar modelo ONNX\n",
    "                save_onnx_model(mql5_files_folder, model, X_train, model_type)\n",
    "                \n",
    "            except Exception as e:\n",
    "                direction_type = f\"{metadata['direction']}_{metadata['type']}\"\n",
    "                print(f\"Error crítico en {direction_type}: {str(e)}\")\n",
    "                raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Procesando dataset de ventas ===\n",
      "Total de operaciones después de eliminar duplicados: 8119\n",
      "Operaciones con profit != 0: 8119\n",
      "Operaciones ganadoras: 3542\n",
      "Operaciones perdedoras: 4577\n",
      "Dataset final preparado: 8119 operaciones, 10 características\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c77d4ad3f91d4cc1a8b1feaaf7f367eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generaciones short_main:   0%|          | 0/5 [00:00<?, ?gen/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c602f214875643d7a449988d2c45b366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generaciones short_meta:   0%|          | 0/5 [00:00<?, ?gen/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, short_main] Fitness: 0.6388422035480859 | Best Fitness: 0.6388422035480859\n",
      "[1, short_main] Fitness Improvement: 0.0000 | Diversity: 1.1693\n",
      "[1, short_main] Normalized Fitness Improvement: 0.0000 | Normalized Diversity: 0.0000\n",
      "[1, short_main] Crossover Rate: 0.9000 | Mutation Rate: 0.1000\n",
      "[1, short_meta] Fitness: 0.6500791383975901 | Best Fitness: 0.6500791383975901\n",
      "[1, short_meta] Fitness Improvement: 0.0000 | Diversity: 1.1664\n",
      "[1, short_meta] Normalized Fitness Improvement: 0.0000 | Normalized Diversity: 0.0000\n",
      "[1, short_meta] Crossover Rate: 0.9000 | Mutation Rate: 0.1000\n",
      "[2, short_main] Fitness: 0.6388422035480859 | Best Fitness: 0.6388422035480859\n",
      "[2, short_main] Fitness Improvement: 0.0000 | Diversity: 1.1622\n",
      "[2, short_main] Normalized Fitness Improvement: 0.0000 | Normalized Diversity: 0.0000\n",
      "[2, short_main] Crossover Rate: 0.5800 | Mutation Rate: 0.1552\n",
      "[2, short_meta] Fitness: 0.6500791383975901 | Best Fitness: 0.6500791383975901\n",
      "[2, short_meta] Fitness Improvement: 0.0000 | Diversity: 1.0740\n",
      "[2, short_meta] Normalized Fitness Improvement: 0.0000 | Normalized Diversity: 0.0000\n",
      "[2, short_meta] Crossover Rate: 0.5800 | Mutation Rate: 0.1552\n",
      "[3, short_main] Fitness: 0.6507936507936507 | Best Fitness: 0.6507936507936507\n",
      "[3, short_main] Fitness Improvement: 0.0120 | Diversity: 1.1875\n",
      "[3, short_main] Normalized Fitness Improvement: 1.0000 | Normalized Diversity: 1.0000\n",
      "[3, short_main] Crossover Rate: 0.1954 | Mutation Rate: 0.8046\n",
      "[3, short_meta] Fitness: 0.6500791383975901 | Best Fitness: 0.6500791383975901\n",
      "[3, short_meta] Fitness Improvement: 0.0000 | Diversity: 1.0190\n",
      "[3, short_meta] Normalized Fitness Improvement: 0.0000 | Normalized Diversity: 0.0000\n",
      "[3, short_meta] Crossover Rate: 0.3737 | Mutation Rate: 0.2408\n",
      "[4, short_main] Fitness: 0.6507936507936507 | Best Fitness: 0.6507936507936507\n",
      "[4, short_main] Fitness Improvement: 0.0000 | Diversity: 1.2323\n",
      "[4, short_main] Normalized Fitness Improvement: 1.0000 | Normalized Diversity: 1.0000\n",
      "[4, short_main] Crossover Rate: 0.1954 | Mutation Rate: 0.8046\n",
      "[5, short_main] Fitness: 0.6507936507936507 | Best Fitness: 0.6507936507936507\n",
      "[5, short_main] Fitness Improvement: 0.0000 | Diversity: 1.2128\n",
      "[5, short_main] Normalized Fitness Improvement: 1.0000 | Normalized Diversity: 0.7211\n",
      "[5, short_main] Crossover Rate: 0.2214 | Mutation Rate: 0.7786\n",
      "####################################################################\n",
      "Mejor puntuación de validación para short_main: 0.6507936507936507\n",
      "Mejores parámetros encontrados para short_main: {'scaler': 'robust', 'reducer': 'kernel_pca_linear', 'reducer__n_components': 9}\n",
      "####################################################################\n",
      "Modelo short_main ONNX exportado correctamente\n",
      "[4, short_meta] Fitness: 0.6500791383975901 | Best Fitness: 0.6500791383975901\n",
      "[4, short_meta] Fitness Improvement: 0.0000 | Diversity: 1.0646\n",
      "[4, short_meta] Normalized Fitness Improvement: 0.0000 | Normalized Diversity: 0.3097\n",
      "[4, short_meta] Crossover Rate: 0.4385 | Mutation Rate: 0.5615\n",
      "[4, short_meta] Early stopping due to no improvement.\n",
      "Best fitness for short_meta: 0.6500791383975901\n",
      "####################################################################\n",
      "Mejor puntuación de validación para short_meta: 0.6500791383975901\n",
      "Mejores parámetros encontrados para short_meta: {'scaler': 'standard', 'reducer': 'kernel_pca_linear', 'reducer__n_components': 2}\n",
      "####################################################################\n",
      "Modelo short_meta ONNX exportado correctamente\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Rutas\n",
    "    common_file_folder = r\"/mnt/c/Users/Administrador/AppData/Roaming/MetaQuotes/Terminal/Common/Files/\"\n",
    "    mql5_files_folder = r'/mnt/c/Users/Administrador/AppData/Roaming/MetaQuotes/Terminal/6C3C6A11D1C3791DD4DBF45421BF8028/MQL5/Files/'\n",
    "    # Definir patrones de archivos para compras y ventas\n",
    "    long_file_pattern = os.path.join(common_file_folder, 'training_dataset_long_*.csv')\n",
    "    short_file_pattern = os.path.join(common_file_folder, 'training_dataset_short_*.csv')\n",
    "    generic_file_pattern = os.path.join(common_file_folder, 'training_dataset_*.csv')\n",
    "    # Encontrar archivos\n",
    "    df_long_file_paths = glob.glob(long_file_pattern)\n",
    "    df_short_file_paths = glob.glob(short_file_pattern)\n",
    "    df_generic_file_paths = glob.glob(generic_file_pattern)\n",
    "    # Inicializar una lista para almacenar los datos\n",
    "    # Procesar todos los datasets\n",
    "    full_data = []\n",
    "    if df_long_file_paths:\n",
    "        full_data.extend(process_files(df_long_file_paths))\n",
    "    if df_short_file_paths:\n",
    "        full_data.extend(process_files(df_short_file_paths))\n",
    "    if not df_long_file_paths and not df_short_file_paths and df_generic_file_paths:\n",
    "        full_data.extend(process_files(df_generic_file_paths))\n",
    "\n",
    "    # Entrenamiento paralelo optimizado\n",
    "    if len(full_data) > 0:\n",
    "        train_models_parallel(full_data, mql5_files_folder)\n",
    "    else:\n",
    "        print(\"No se encontraron datasets válidos para entrenar\")\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##########\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
