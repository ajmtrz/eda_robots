{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.decomposition import KernelPCA, TruncatedSVD\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from genetic_algorithm import GeneticAlgorithmCV\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from onnxmltools.convert.xgboost.operator_converters.XGBoost import convert_xgboost\n",
    "from onnxmltools.convert import convert_xgboost as convert_xgboost_booster\n",
    "from onnxmltools.convert.lightgbm.operator_converters.LightGbm import convert_lightgbm\n",
    "from skl2onnx import convert_sklearn, update_registered_converter\n",
    "from skl2onnx.common.shape_calculator import calculate_linear_classifier_output_shapes\n",
    "from skl2onnx.common.data_types import FloatTensorType, Int64TensorType\n",
    "from skl2onnx._parse import _apply_zipmap, _get_sklearn_operator_name\n",
    "from onnx.helper import get_attribute_value\n",
    "from catboost.utils import convert_to_onnx_object\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def extract_action(filepath):\n",
    "    match = re.search(r'(long|short)', filepath)\n",
    "    return match.group(0) if match else None\n",
    "\n",
    "# Crear datasets de entrenamiento\n",
    "def create_training_dataset(df, action_type):\n",
    "    print(f\"=== Procesando dataset de {'compras' if action_type == 'long' else 'ventas'} ===\")\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"Total de operaciones después de eliminar duplicados: {len(df)}\")\n",
    "    # Filtrar las operaciones con profit != 0\n",
    "    df_trade = df[df['profit'] != 0].copy()\n",
    "    print(f\"Operaciones con profit != 0: {len(df_trade)}\")\n",
    "    # Añadir la columna 'target' basada en el profit\n",
    "    df_trade['target'] = df_trade['profit'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    num_ganadoras = df_trade['target'].sum()\n",
    "    num_perdedoras = len(df_trade) - num_ganadoras\n",
    "    print(f\"Operaciones ganadoras: {int(num_ganadoras)}\")\n",
    "    print(f\"Operaciones perdedoras: {int(num_perdedoras)}\")\n",
    "    # Eliminar posibles missings\n",
    "    if df_trade.isna().values.any():\n",
    "        num_missings = df_trade.isna().sum().sum()\n",
    "        print(f\"Valores faltantes encontrados: {num_missings}\")\n",
    "        df_trade = df_trade.dropna()\n",
    "        print(f\"Total de operaciones después de eliminar missings: {len(df_trade)}\")\n",
    "    df_training = df_trade.copy(deep=True)\n",
    "    # Seleccionar las columnas necesarias (todas menos las dos últimas para el conjunto principal)\n",
    "    feature_columns = df_training.columns[:-2]\n",
    "    df_training = df_training[feature_columns.tolist() + ['target']]\n",
    "    # Preparación de los datos de entrenamiento\n",
    "    X_train = df_training.drop(columns='target').values.astype('float')\n",
    "    y_train = df_training['target'].values.astype('int')\n",
    "    print(f\"Dataset final preparado: {X_train.shape[0]} operaciones, {X_train.shape[1]} características\")\n",
    "    return X_train, y_train\n",
    "\n",
    "def meta_label_data_multi_bootstrap_oob(\n",
    "    X, y, \n",
    "    models_number=25, \n",
    "    # En vez de un único valor, podemos recibir una lista de valores a probar:\n",
    "    fractions_to_try=None  \n",
    "):\n",
    "    \"\"\"\n",
    "    Realiza OOB bootstrap + detección de malas muestras.\n",
    "    Además, optimiza automáticamente 'bad_samples_fraction' entre varios valores\n",
    "    usando la métrica OOB.\n",
    "    \"\"\"\n",
    "    if fractions_to_try is None:\n",
    "        # Por defecto probamos varios valores\n",
    "        fractions_to_try = [0.5, 0.6, 0.7, 0.8]\n",
    "\n",
    "    def sample_random_hparams_catboost():\n",
    "        # Devuelve parámetros aleatorios para CatBoostClassifier\n",
    "        return {\n",
    "            'iterations': random.randint(100, 500),\n",
    "            'max_depth': random.randint(3, 10),\n",
    "            'learning_rate': random.uniform(0.1, 0.5),\n",
    "            'l2_leaf_reg': random.uniform(0.0, 1.0),\n",
    "            'min_data_in_leaf': random.randint(1, 10)\n",
    "        }\n",
    "\n",
    "    def sample_random_hparams_xgb():\n",
    "        # Devuelve parámetros aleatorios para XGBClassifier\n",
    "        return {\n",
    "            'n_estimators': random.randint(50, 500),\n",
    "            'max_depth': random.randint(3, 10),\n",
    "            'eta': random.uniform(0.1, 0.5),\n",
    "            'gamma': random.uniform(0.0, 0.5),\n",
    "            'subsample': random.uniform(0.5, 1.0),\n",
    "            'colsample_bytree': random.uniform(0.5, 1.0)\n",
    "        }\n",
    "\n",
    "    def sample_random_hparams_lgbm():\n",
    "        # Devuelve parámetros aleatorios para LGBMClassifier\n",
    "        return {\n",
    "            'n_estimators': random.randint(50, 500),\n",
    "            'max_depth': random.randint(3, 10),\n",
    "            'learning_rate': random.uniform(0.1, 0.5),\n",
    "            'min_child_samples': random.randint(3, 10)\n",
    "        }\n",
    "    \n",
    "    # Podemos ajustar esta función para no depender directamente de 'bad_samples_fraction'\n",
    "    # y usarla solo para el cálculo de percentiles:\n",
    "    def safe_threshold(series, fraction):\n",
    "        non_zero = series[series > 0]\n",
    "        if len(non_zero) >= 5:\n",
    "            return np.percentile(non_zero, 75) * fraction\n",
    "        elif len(non_zero) > 0:\n",
    "            return np.median(non_zero) * fraction\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    df = pd.DataFrame(X)\n",
    "    df['target'] = y\n",
    "    df_no_target = df.drop(columns='target')\n",
    "    scaler = RobustScaler()\n",
    "    df_scaled_np = scaler.fit_transform(df_no_target)  \n",
    "    df_scaled = pd.DataFrame(df_scaled_np, index=df.index, columns=df_no_target.columns)\n",
    "    for col in df_no_target.columns:\n",
    "        df[col] = df_scaled[col]\n",
    "    # Contadores\n",
    "    oob_counts = pd.Series(0, index=df.index)\n",
    "    error_counts_0 = pd.Series(0, index=df.index)\n",
    "    error_counts_1 = pd.Series(0, index=df.index)\n",
    "\n",
    "    # ===============================\n",
    "    # (1) Paso: Generar error_counts con \"models_number\" iteraciones\n",
    "    # ===============================\n",
    "    for i in range(models_number):\n",
    "        # Fracción de bootstrap para este ensemble\n",
    "        frac_bootstrap = random.uniform(0.4, 0.6)\n",
    "        train_sample = df.sample(frac=frac_bootstrap, replace=True, random_state=None)\n",
    "        \n",
    "        val_sample = df.loc[~df.index.isin(train_sample.index)]\n",
    "\n",
    "        # Hiperparámetros de cada modelo base\n",
    "        hparams_cat = sample_random_hparams_catboost()\n",
    "        hparams_xgb = sample_random_hparams_xgb()\n",
    "        hparams_lgbm = sample_random_hparams_lgbm()\n",
    "\n",
    "        base_models = [\n",
    "            ('catboost', CatBoostClassifier(task_type=\"CPU\", verbose=False, **hparams_cat)),\n",
    "            ('xgboost', XGBClassifier(verbosity=0, **hparams_xgb)),\n",
    "            ('lightgbm', LGBMClassifier(verbosity=-1 ,**hparams_lgbm))\n",
    "        ]\n",
    "\n",
    "        model = Pipeline([\n",
    "            ('ensemble', VotingClassifier(\n",
    "                estimators=base_models,\n",
    "                voting='soft',\n",
    "                flatten_transform=False,\n",
    "                n_jobs=1\n",
    "            ))\n",
    "        ])\n",
    "\n",
    "        model.fit(train_sample.drop(columns='target'), train_sample['target'])\n",
    "        \n",
    "        if len(val_sample) == 0:\n",
    "            continue\n",
    "        \n",
    "        pred_proba = model.named_steps['ensemble'].predict_proba(val_sample.drop(columns='target'))[:, 1]\n",
    "        \n",
    "        # Ajuste dinámico del threshold en val_sample para maximizar la accuracy local\n",
    "        if len(val_sample['target'].unique()) > 1:\n",
    "            best_acc = 0\n",
    "            best_thr = 0.5\n",
    "            for thr_candidate in np.linspace(0, 1, 21):\n",
    "                pred_temp = (pred_proba >= thr_candidate).astype(int)\n",
    "                acc_temp = (val_sample['target'].values == pred_temp).mean()\n",
    "                if acc_temp > best_acc:\n",
    "                    best_acc = acc_temp\n",
    "                    best_thr = thr_candidate\n",
    "            pred_labels = (pred_proba >= best_thr).astype(int)\n",
    "        else:\n",
    "            pred_labels = (pred_proba >= 0.5).astype(int)\n",
    "        \n",
    "        val_sample = val_sample.copy()\n",
    "        val_sample['pred'] = pred_labels\n",
    "        \n",
    "        # Marcar errores\n",
    "        val_sample_0 = val_sample[val_sample['target'] == 0]\n",
    "        val_sample_1 = val_sample[val_sample['target'] == 1]\n",
    "        diff_0 = val_sample_0.index[val_sample_0['target'] != val_sample_0['pred']]\n",
    "        diff_1 = val_sample_1.index[val_sample_1['target'] != val_sample_1['pred']]\n",
    "        \n",
    "        oob_counts.loc[val_sample.index] += 1\n",
    "        error_counts_0.loc[diff_0] += 1\n",
    "        error_counts_1.loc[diff_1] += 1\n",
    "\n",
    "    # ===============================\n",
    "    # (2) Paso: Calculamos el ratio de error para cada muestra\n",
    "    # ===============================\n",
    "    to_mark_0 = (error_counts_0 / oob_counts.replace(0, 1)).fillna(0)\n",
    "    to_mark_1 = (error_counts_1 / oob_counts.replace(0, 1)).fillna(0)\n",
    "\n",
    "    # ===============================\n",
    "    # (3) Paso: Probar varios 'bad_samples_fraction' y quedarnos con el mejor\n",
    "    # ===============================\n",
    "\n",
    "    best_fraction = None\n",
    "    best_score = np.inf  # Usamos \"score = media de error OOB\" -> cuanto menor, mejor\n",
    "\n",
    "    for frac in fractions_to_try:\n",
    "        # Definir umbrales para esta fracción\n",
    "        threshold_0 = safe_threshold(to_mark_0, frac)\n",
    "        threshold_1 = safe_threshold(to_mark_1, frac)\n",
    "\n",
    "        # Marcar “malas” muestras\n",
    "        marked_0 = to_mark_0[to_mark_0 > threshold_0].index if len(to_mark_0) else []\n",
    "        marked_1 = to_mark_1[to_mark_1 > threshold_1].index if len(to_mark_1) else []\n",
    "        all_bad = pd.Index(marked_0).union(marked_1)\n",
    "\n",
    "        # Muestras \"buenas\"\n",
    "        good_mask = ~df.index.isin(all_bad)\n",
    "\n",
    "        # (a) Calculamos la \"media de error OOB\" solo en las muestras buenas\n",
    "        #     (Así sabemos qué tan limpias quedan)\n",
    "        error_ratios_good = []\n",
    "        for idx in df[good_mask].index:\n",
    "            if df.loc[idx, 'target'] == 0:\n",
    "                error_ratios_good.append(to_mark_0[idx])\n",
    "            else:\n",
    "                error_ratios_good.append(to_mark_1[idx])\n",
    "        \n",
    "        mean_error_good = np.mean(error_ratios_good) if len(error_ratios_good) else 1.0\n",
    "\n",
    "        # Ver si mejora\n",
    "        # Mientras más bajo, mejor (menos error OOB en las muestras buenas)\n",
    "        if mean_error_good < best_score:\n",
    "            best_score = mean_error_good\n",
    "            best_fraction = frac\n",
    "\n",
    "    # ===============================\n",
    "    # (4) Paso: Con la mejor fracción, hacemos el filtrado definitivo\n",
    "    # ===============================\n",
    "    threshold_0 = safe_threshold(to_mark_0, best_fraction)\n",
    "    threshold_1 = safe_threshold(to_mark_1, best_fraction)\n",
    "    \n",
    "    marked_0 = to_mark_0[to_mark_0 > threshold_0].index if len(to_mark_0) else []\n",
    "    marked_1 = to_mark_1[to_mark_1 > threshold_1].index if len(to_mark_1) else []\n",
    "    all_bad = pd.Index(marked_0).union(marked_1)\n",
    "\n",
    "    df['meta_labels'] = 1\n",
    "    df.loc[all_bad, 'meta_labels'] = 0\n",
    "    \n",
    "    X_main = df.loc[df['meta_labels'] == 1].drop(columns=['target', 'meta_labels'])\n",
    "    y_main = df.loc[df['meta_labels'] == 1, 'target']\n",
    "    \n",
    "    X_meta = df.drop(columns=['target', 'meta_labels'])\n",
    "    y_meta = df['meta_labels']\n",
    "    \n",
    "    print(f\"Malas muestras filtradas: {len(df) - len(X_main)} (con fraction={best_fraction})\")\n",
    "    return (X_main, y_main), (X_meta, y_meta)\n",
    "\n",
    "# Procesamiento de archivos (modificado para devolver 4 componentes)\n",
    "def process_files(file_paths):\n",
    "    processed_data = []\n",
    "    for file_path in file_paths:\n",
    "        model_type = extract_action(file_path)  # 'long' o 'short'\n",
    "        df = pd.read_csv(file_path)\n",
    "        # (1) Generar datos base\n",
    "        X_train, y_train = create_training_dataset(df, model_type)\n",
    "        # (2) Aplicar meta-labeling (devuelve 2 conjuntos)\n",
    "        (X_main, y_main), (X_meta, y_meta) = meta_label_data_multi_bootstrap_oob(\n",
    "            X_train, y_train,\n",
    "            models_number=100,\n",
    "            fractions_to_try=[0.8]\n",
    "        )\n",
    "        processed_data.append({\n",
    "            'direction': model_type,\n",
    "            'main': (X_main, y_main),\n",
    "            'meta': (X_meta, y_meta)\n",
    "        })\n",
    "    return processed_data\n",
    "\n",
    "# Train model functions\n",
    "def train_classifier(X_train, y_train, model_type):\n",
    "    # Definir algoritmo de validación cruzada\n",
    "    cv = StratifiedKFold(n_splits=3)\n",
    "    # Definir Pipeline\n",
    "    base_models = [\n",
    "        ('catboost', CatBoostClassifier(task_type=\"CPU\", verbose=False)),\n",
    "        ('xgboost', XGBClassifier(verbosity=0)),\n",
    "        ('lightgbm', LGBMClassifier(verbosity=-1))\n",
    "    ]\n",
    "    pipeline = Pipeline([\n",
    "        ('ensemble', VotingClassifier(\n",
    "            estimators=base_models,\n",
    "            voting='soft',\n",
    "            flatten_transform=False,\n",
    "            n_jobs=1\n",
    "        ))\n",
    "    ])\n",
    "    # Definir mapa de estimadores\n",
    "    # estimator_map = {\n",
    "    #     'scaler': {\n",
    "    #         'standard': RobustScaler(),\n",
    "    #         'robust': RobustScaler(),\n",
    "    #         'none': 'passthrough'\n",
    "    #     },\n",
    "    #     'reducer': {\n",
    "    #         'kernel_pca_rbf': KernelPCA(kernel='rbf'),\n",
    "    #         'kernel_pca_linear': KernelPCA(kernel='linear'),\n",
    "    #         'truncated_svd': TruncatedSVD(),\n",
    "    #         'none': 'passthrough'\n",
    "    #     }\n",
    "    # }\n",
    "    # Definir espacio de hiperparámetros\n",
    "    param_grid = {\n",
    "        'ensemble__catboost__iterations': {'type': 'int', 'low': 100, 'high': 500},\n",
    "        'ensemble__catboost__max_depth': {'type': 'int', 'low': 3, 'high': 10},\n",
    "        'ensemble__catboost__learning_rate': {'type': 'float', 'low': 0.1, 'high': 0.5},\n",
    "        'ensemble__catboost__l2_leaf_reg': {'type': 'float', 'low': 0.0, 'high': 1.0},\n",
    "        'ensemble__catboost__min_data_in_leaf': {'type': 'int', 'low': 3, 'high': 10},\n",
    "        'ensemble__xgboost__n_estimators': {'type': 'int', 'low': 50, 'high': 500},\n",
    "        'ensemble__xgboost__max_depth': {'type': 'int', 'low': 3, 'high': 10},\n",
    "        'ensemble__xgboost__eta': {'type': 'float', 'low': 0.1, 'high': 0.5},\n",
    "        'ensemble__xgboost__gamma': {'type': 'float', 'low': 0.0, 'high': 0.5},\n",
    "        'ensemble__xgboost__subsample': {'type': 'float', 'low': 0.5, 'high': 1.0},\n",
    "        'ensemble__xgboost__colsample_bytree': {'type': 'float', 'low': 0.5, 'high': 1.0},\n",
    "        'ensemble__lightgbm__n_estimators': {'type': 'int', 'low': 50, 'high': 500},\n",
    "        'ensemble__lightgbm__max_depth': {'type': 'int', 'low': 3, 'high': 10},\n",
    "        'ensemble__lightgbm__learning_rate': {'type': 'float', 'low': 0.1, 'high': 0.5},\n",
    "        'ensemble__lightgbm__min_child_samples': {'type': 'int', 'low': 3, 'high': 10}\n",
    "    }\n",
    "    try:\n",
    "        # Entrenar el modelo utilizando el algoritmo genético\n",
    "        ga_search = GeneticAlgorithmCV(\n",
    "            model_type=model_type,\n",
    "            pipeline=pipeline,\n",
    "            param_grid=param_grid,\n",
    "            #estimator_map=estimator_map,\n",
    "            cv=cv,\n",
    "            pop_size=25,\n",
    "            generations=5,\n",
    "            early_stopping_rounds=3,\n",
    "            crossover_initial=0.9,\n",
    "            crossover_end=0.1,\n",
    "            mutation_initial=0.1,\n",
    "            mutation_end=0.9,\n",
    "            elitism=True,\n",
    "            elite_size=5,\n",
    "            tournament_size=3,\n",
    "            n_random=5,\n",
    "            n_jobs=1,\n",
    "            verbose=True,\n",
    "        )\n",
    "        ga_search.fit(X_train, y_train)\n",
    "    except Exception as e:\n",
    "        print(f\"Error en traing model {model_type}: {e}\")\n",
    "        raise\n",
    "    # Obtener los mejores parámetros y el mejor estimador\n",
    "    print(\"####################################################################\")\n",
    "    print(f\"Mejor puntuación de validación para {model_type}: {ga_search.best_score_}\")\n",
    "    print(f\"Mejores parámetros encontrados para {model_type}: {ga_search.best_params_full_}\")\n",
    "    print(\"####################################################################\")\n",
    "    # Retornar mejor estimador\n",
    "    return ga_search.best_estimator_\n",
    "\n",
    "# ONNX para Pipeline con Catboost\n",
    "def skl2onnx_parser_castboost_classifier(scope, model, inputs, custom_parsers=None):\n",
    "    options = scope.get_options(model, dict(zipmap=True))\n",
    "    no_zipmap = isinstance(options[\"zipmap\"], bool) and not options[\"zipmap\"]\n",
    "    \n",
    "    alias = _get_sklearn_operator_name(type(model))\n",
    "    this_operator = scope.declare_local_operator(alias, model)\n",
    "    this_operator.inputs = inputs\n",
    "    \n",
    "    label_variable = scope.declare_local_variable(\"label\", Int64TensorType())\n",
    "    probability_tensor_variable = scope.declare_local_variable(\"probabilities\", FloatTensorType())\n",
    "    \n",
    "    this_operator.outputs.append(label_variable)\n",
    "    this_operator.outputs.append(probability_tensor_variable)\n",
    "    \n",
    "    return _apply_zipmap(options[\"zipmap\"], scope, model, inputs[0].type, this_operator.outputs)\n",
    "\n",
    "def skl2onnx_convert_catboost(scope, operator, container):\n",
    "    onx = convert_to_onnx_object(operator.raw_operator)\n",
    "    node = onx.graph.node[0]\n",
    "    \n",
    "    container.add_node(\n",
    "        node.op_type,\n",
    "        [operator.inputs[0].full_name],\n",
    "        [operator.outputs[0].full_name, operator.outputs[1].full_name],\n",
    "        op_domain=node.domain,\n",
    "        **{att.name: get_attribute_value(att) for att in node.attribute}\n",
    "    )\n",
    "\n",
    "def save_onnx_model(mql5_files_folder, model, X, model_type):\n",
    "    update_registered_converter(\n",
    "        CatBoostClassifier,\n",
    "        \"CatBoostClassifier\",\n",
    "        calculate_linear_classifier_output_shapes,\n",
    "        skl2onnx_convert_catboost,\n",
    "        parser=skl2onnx_parser_castboost_classifier,\n",
    "        options={\"nocl\": [True, False], \"zipmap\": [True, False]}\n",
    "    )\n",
    "    update_registered_converter(\n",
    "        XGBClassifier,\n",
    "        'XGBClassifier',\n",
    "        calculate_linear_classifier_output_shapes,\n",
    "        convert_xgboost,\n",
    "        options={'nocl': [True, False], 'zipmap': [True, False]}\n",
    "    )\n",
    "    update_registered_converter(\n",
    "        LGBMClassifier,\n",
    "        'LGBMClassifier',\n",
    "        calculate_linear_classifier_output_shapes,\n",
    "        convert_lightgbm,\n",
    "        options={'nocl': [True, False], 'zipmap': [True, False]}\n",
    "    )\n",
    "    try:\n",
    "        # Define el tipo de entrada\n",
    "        initial_type = [('input', FloatTensorType([None, X.shape[1]]))]\n",
    "        \n",
    "        # Convierte el pipeline completo\n",
    "        model_onnx = convert_sklearn(\n",
    "            model,\n",
    "            initial_types=initial_type,\n",
    "            target_opset={\"\": 18, \"ai.onnx.ml\": 2},\n",
    "            options={id(model.named_steps['ensemble']): {'zipmap': True}}\n",
    "        )\n",
    "        \n",
    "        # Guarda el modelo\n",
    "        with open(os.path.join(mql5_files_folder, f\"model_{model_type}.onnx\"), \"wb\") as f:\n",
    "            f.write(model_onnx.SerializeToString())\n",
    "            \n",
    "        print(f\"Modelo {model_type} ONNX exportado correctamente\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error en exportar el modelo {model_type}: {e}\")\n",
    "        raise\n",
    "\n",
    "def train_models_parallel(data_list, mql5_files_folder):\n",
    "    # Diccionario para rastrear metadatos de cada futuro\n",
    "    future_metadata = {}\n",
    "    futures = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        # Paso 1: Enviar todas las tareas de entrenamiento\n",
    "        for data in data_list:\n",
    "            direction = data['direction']\n",
    "            \n",
    "            # Modelo Principal (main)\n",
    "            X_main, y_main = data['main']\n",
    "            scaler_main = RobustScaler()\n",
    "            X_main_scaled = scaler_main.fit_transform(X_main)\n",
    "            future_main = executor.submit(\n",
    "                train_classifier, \n",
    "                X_main_scaled, \n",
    "                y_main, \n",
    "                f\"{direction}_main\"\n",
    "            )\n",
    "            future_metadata[future_main] = {\n",
    "                'type': 'main',\n",
    "                'direction': direction,\n",
    "                'X_train': X_main,  \n",
    "                'scaler': scaler_main\n",
    "            }\n",
    "            futures.append(future_main)\n",
    "            \n",
    "            # Meta-Modelo (validación causal)\n",
    "            X_meta, y_meta = data['meta']\n",
    "            scaler_meta = RobustScaler()\n",
    "            X_meta_scaled = scaler_meta.fit_transform(X_meta)\n",
    "            future_meta = executor.submit(\n",
    "                train_classifier,\n",
    "                X_meta_scaled,\n",
    "                y_meta.astype(int), \n",
    "                f\"{direction}_meta\" \n",
    "            )\n",
    "            future_metadata[future_meta] = {\n",
    "                'type': 'meta',\n",
    "                'direction': direction,\n",
    "                'X_train': X_meta,  \n",
    "                'scaler': scaler_meta\n",
    "            }\n",
    "            futures.append(future_meta)\n",
    "        \n",
    "        # Paso 2: Procesar resultados conforme se completan\n",
    "        for future in as_completed(futures):\n",
    "            metadata = future_metadata[future]\n",
    "            try:\n",
    "                best_estimator = future.result()\n",
    "                model_type = f\"{metadata['direction']}_{metadata['type']}\"\n",
    "                X_train = metadata['X_train']\n",
    "                scaler = metadata['scaler']\n",
    "                # Reconstruir pipeline\n",
    "                final_pipeline = Pipeline([\n",
    "                    (\"scaler\", scaler),\n",
    "                    (\"ensemble\", best_estimator.named_steps['ensemble'])\n",
    "                ])\n",
    "                # Guardar modelo ONNX\n",
    "                save_onnx_model(mql5_files_folder, final_pipeline, X_train, model_type)\n",
    "                \n",
    "            except Exception as e:\n",
    "                direction_type = f\"{metadata['direction']}_{metadata['type']}\"\n",
    "                print(f\"Error crítico en {direction_type}: {str(e)}\")\n",
    "                raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Rutas\n",
    "    common_file_folder = r\"/mnt/c/Users/Administrador/AppData/Roaming/MetaQuotes/Terminal/Common/Files/\"\n",
    "    mql5_files_folder = r'/mnt/c/Users/Administrador/AppData/Roaming/MetaQuotes/Terminal/6C3C6A11D1C3791DD4DBF45421BF8028/MQL5/Files/'\n",
    "    # Definir patrones de archivos para compras y ventas\n",
    "    long_file_pattern = os.path.join(common_file_folder, 'training_dataset_long_*.csv')\n",
    "    short_file_pattern = os.path.join(common_file_folder, 'training_dataset_short_*.csv')\n",
    "    generic_file_pattern = os.path.join(common_file_folder, 'training_dataset_*.csv')\n",
    "    # Encontrar archivos\n",
    "    df_long_file_paths = glob.glob(long_file_pattern)\n",
    "    df_short_file_paths = glob.glob(short_file_pattern)\n",
    "    df_generic_file_paths = glob.glob(generic_file_pattern)\n",
    "    # Procesar todos los datasets\n",
    "    full_data = []\n",
    "    if df_long_file_paths:\n",
    "        full_data.extend(process_files(df_long_file_paths))\n",
    "    if df_short_file_paths:\n",
    "        full_data.extend(process_files(df_short_file_paths))\n",
    "    if not df_long_file_paths and not df_short_file_paths and df_generic_file_paths:\n",
    "        full_data.extend(process_files(df_generic_file_paths))\n",
    "    # Entrenamiento paralelo optimizado\n",
    "    if len(full_data) > 0:\n",
    "        train_models_parallel(full_data, mql5_files_folder)\n",
    "    else:\n",
    "        print(\"No se encontraron datasets válidos para entrenar\")\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##########\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
