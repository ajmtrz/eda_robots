{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import copy\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, Any\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from modules.labeling_lib import get_prices, get_labels_one_direction, get_features\n",
    "from modules.tester_lib import test_model_one_direction\n",
    "import optuna\n",
    "from optuna.pruners import SuccessiveHalvingPruner, HyperbandPruner\n",
    "from modules.export_lib import export_model_to_ONNX\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "def bootstrap_oob_identification(X, y, n_models=25):\n",
    "    \"\"\"Identifica muestras problemáticas usando bootstrap OOB\"\"\"\n",
    "    # Contadores para errores\n",
    "    oob_counts = pd.Series(0, index=X.index)\n",
    "    error_counts_0 = pd.Series(0, index=X.index)\n",
    "    error_counts_1 = pd.Series(0, index=X.index)\n",
    "    \n",
    "    for _ in range(n_models):\n",
    "        # Bootstrap sample\n",
    "        frac_bootstrap = random.uniform(0.4, 0.6)\n",
    "        train_idx = X.sample(frac=frac_bootstrap, replace=True, random_state=None).index\n",
    "        val_idx = X.index.difference(train_idx)\n",
    "        \n",
    "        if len(val_idx) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Entrenar modelo\n",
    "        model = CatBoostClassifier(\n",
    "            iterations=random.randint(100, 500),\n",
    "            depth=random.randint(3, 10),\n",
    "            learning_rate=random.uniform(0.1, 0.5),\n",
    "            l2_leaf_reg=random.uniform(0.0, 1.0),\n",
    "            verbose=False\n",
    "        )\n",
    "        model.fit(X.loc[train_idx], y.loc[train_idx])\n",
    "        \n",
    "        # Predicciones OOB\n",
    "        pred_proba = model.predict_proba(X.loc[val_idx])[:, 1]\n",
    "        pred_labels = (pred_proba >= 0.5).astype(int)\n",
    "        \n",
    "        # Encontrar diferencias para cada clase\n",
    "        val_y = y.loc[val_idx]\n",
    "        val_0_idx = val_idx[val_y == 0]\n",
    "        val_1_idx = val_idx[val_y == 1]\n",
    "        \n",
    "        diff_0 = val_0_idx[pred_labels[val_y == 0] != 0]\n",
    "        diff_1 = val_1_idx[pred_labels[val_y == 1] != 1]\n",
    "        \n",
    "        oob_counts.loc[val_idx] += 1\n",
    "        error_counts_0.loc[diff_0] += 1\n",
    "        error_counts_1.loc[diff_1] += 1\n",
    "    \n",
    "    return error_counts_0, error_counts_1, oob_counts\n",
    "\n",
    "def optimize_bad_samples_threshold(error_counts_0, error_counts_1, oob_counts, fractions_to_try=[0.5, 0.6, 0.7, 0.8]):\n",
    "    \"\"\"Optimiza el umbral para identificar muestras problemáticas\"\"\"\n",
    "    to_mark_0 = (error_counts_0 / oob_counts.replace(0, 1)).fillna(0)\n",
    "    to_mark_1 = (error_counts_1 / oob_counts.replace(0, 1)).fillna(0)\n",
    "    \n",
    "    best_fraction = None\n",
    "    best_score = np.inf\n",
    "    \n",
    "    for frac in fractions_to_try:\n",
    "        # Calcular umbrales\n",
    "        threshold_0 = np.percentile(to_mark_0[to_mark_0 > 0], 75) * frac if len(to_mark_0[to_mark_0 > 0]) else 0\n",
    "        threshold_1 = np.percentile(to_mark_1[to_mark_1 > 0], 75) * frac if len(to_mark_1[to_mark_1 > 0]) else 0\n",
    "        \n",
    "        # Marcar muestras problemáticas\n",
    "        marked_0 = to_mark_0[to_mark_0 > threshold_0].index\n",
    "        marked_1 = to_mark_1[to_mark_1 > threshold_1].index\n",
    "        all_bad = pd.Index(marked_0).union(marked_1)\n",
    "        \n",
    "        # Calcular score (media de error en muestras buenas)\n",
    "        good_mask = ~to_mark_0.index.isin(all_bad)\n",
    "        error_ratios_good = []\n",
    "        for idx in to_mark_0[good_mask].index:\n",
    "            if to_mark_0[idx] > 0:\n",
    "                error_ratios_good.append(to_mark_0[idx])\n",
    "            if to_mark_1[idx] > 0:\n",
    "                error_ratios_good.append(to_mark_1[idx])\n",
    "        \n",
    "        mean_error_good = np.mean(error_ratios_good) if error_ratios_good else 1.0\n",
    "        \n",
    "        if mean_error_good < best_score:\n",
    "            best_score = mean_error_good\n",
    "            best_fraction = frac\n",
    "    \n",
    "    return best_fraction, best_score\n",
    "\n",
    "def objective(trial: optuna.trial.Trial, base_hp: dict, study=None) -> float:\n",
    "    def calc_score(fwd: float, bwd: float, eps: float = 1e-9) -> float:\n",
    "        if (fwd is None or bwd is None or\n",
    "            not np.isfinite(fwd) or not np.isfinite(bwd) or\n",
    "            fwd <= 0 or bwd <= 0):\n",
    "            return -1.0\n",
    "        mean = 0.5 * (fwd + bwd)\n",
    "        delta = abs(fwd - bwd) / max(abs(fwd), abs(bwd), eps)\n",
    "        score = mean * (1.0 - delta)\n",
    "        return score\n",
    "\n",
    "    best_combined_score = -math.inf\n",
    "    hp = {k: copy.deepcopy(v) for k, v in base_hp.items() if k != 'base_df'}\n",
    "    gc.collect()\n",
    "    \n",
    "    # Optimización de períodos para el modelo principal\n",
    "    n_periods = trial.suggest_int('n_periods', 5, 15)\n",
    "    periods = []\n",
    "    for i in range(n_periods):\n",
    "        period = trial.suggest_int(f'period_{i}', 5, 200, log=True)\n",
    "        periods.append(period)\n",
    "    hp['periods_main'] = sorted(list(set(periods)))\n",
    "\n",
    "    # Selección de estadísticas para el modelo principal\n",
    "    stat_choices = [\n",
    "        \"std\", \"skew\", \"kurt\", \"zscore\", \"range\", \"mad\", \"entropy\", \n",
    "        \"slope\", \"momentum\", \"fractal\", \"hurst\", \"autocorr\", \"max_dd\", \n",
    "        \"sharpe\", \"fisher\", \"chande\", \"var\", \"approx_entropy\", \n",
    "        \"eff_ratio\", \"corr_skew\", \"jump_vol\", \"vol_skew\"\n",
    "    ]\n",
    "    stats = trial.suggest_int('stats_main', 1, 5)\n",
    "    selected_main_stats = []\n",
    "    for i in range(stats):\n",
    "        stat = trial.suggest_categorical(f'stat_{i}', stat_choices)\n",
    "        selected_main_stats.append(stat)\n",
    "    selected_main_stats = list(set(selected_main_stats))\n",
    "    if len(selected_main_stats) == 1 and (\"fractal\" in selected_main_stats or \"hurst\" in selected_main_stats or \"kurt\" in selected_main_stats):\n",
    "        remaining_stats = [s for s in stat_choices if s != \"fractal\" and s != \"hurst\" and s != \"kurt\"]\n",
    "        additional_stat = trial.suggest_categorical('additional_stat', remaining_stats)\n",
    "        selected_main_stats.append(additional_stat)\n",
    "    hp[\"stats_main\"] = selected_main_stats\n",
    "\n",
    "    # Optimización de parámetros de etiquetado\n",
    "    hp['label_min'] = trial.suggest_int('label_min', 1, 5)\n",
    "    hp['label_max'] = trial.suggest_int('label_max', hp['label_min']+5, 30)\n",
    "    hp['markup'] = trial.suggest_float(\"markup\", 0.1, 0.4)\n",
    "\n",
    "    # CatBoost principal - Mayor capacidad de aprendizaje\n",
    "    hp['cat_main_iterations'] = trial.suggest_int('cat_main_iterations', 300, 2000, step=100)\n",
    "    hp['cat_main_depth'] = trial.suggest_int('cat_main_depth', 6, 12)\n",
    "    hp['cat_main_learning_rate'] = trial.suggest_float('cat_main_learning_rate', 0.005, 0.4, log=True)\n",
    "    hp['cat_main_l2_leaf_reg'] = trial.suggest_float('cat_main_l2_leaf_reg', 0.5, 10.0)\n",
    "    hp['cat_main_early_stopping'] = trial.suggest_int('cat_main_early_stopping', 10, 50, step=10)\n",
    "    #hp['cat_main_rsm'] = trial.suggest_float('cat_main_rsm', 0.1, 1.0)\n",
    "\n",
    "    # CatBoost meta - Enfoque en precisión\n",
    "    hp['cat_meta_iterations'] = trial.suggest_int('cat_meta_iterations', 200, 1000, step=100)\n",
    "    hp['cat_meta_depth'] = trial.suggest_int('cat_meta_depth', 5, 10)\n",
    "    hp['cat_meta_learning_rate'] = trial.suggest_float('cat_meta_learning_rate', 0.01, 0.3, log=True)\n",
    "    hp['cat_meta_l2_leaf_reg'] = trial.suggest_float('cat_meta_l2_leaf_reg', 0.5, 8.0)\n",
    "    hp['cat_meta_early_stopping'] = trial.suggest_int('cat_meta_early_stopping', 10, 50, step=10)\n",
    "\n",
    "    # Optimización del número de meta-learners y fracción de muestras malas\n",
    "    hp['n_meta_learners'] = trial.suggest_int('n_meta_learners', 1, 10)\n",
    "    hp['bad_samples_fraction'] = trial.suggest_float('bad_samples_fraction', 0.5, 1.0)\n",
    "\n",
    "    # Obtener datos y características\n",
    "    full_ds = get_labels_one_direction(get_features(get_prices(hp), hp), \n",
    "                                     markup=hp['markup'], \n",
    "                                     min=hp['label_min'],\n",
    "                                     max=hp['label_max'],\n",
    "                                     direction=hp['direction'])\n",
    "\n",
    "    # Dividir en períodos de entrenamiento, backward testing y forward testing\n",
    "    train_mask = (full_ds.index > hp['backward']) & (full_ds.index < hp['forward'])\n",
    "    oos_mask = (full_ds.index >= hp['forward']) & (full_ds.index < hp['full_forward'])\n",
    "\n",
    "    # Extraer características y asegurar el orden consistente\n",
    "    feature_cols = sorted(full_ds.filter(regex='_feature$').columns)\n",
    "    ds_train = full_ds.loc[train_mask, feature_cols]\n",
    "    y_train = full_ds.loc[train_mask]['labels'].astype('int16')\n",
    "    ds_oos = full_ds.loc[oos_mask, feature_cols]\n",
    "    ds_train_val = ds_train.iloc[-len(ds_oos):]\n",
    "\n",
    "    # Clustering con ventana deslizante\n",
    "    assert ds_train.index.max() < hp['forward'], \\\n",
    "       \"¡ds_train contiene fechas posteriores al límite forward!\"\n",
    "    assert ds_oos.index.min() >= hp['forward'], \\\n",
    "        \"¡ds_oos incluye datos dentro del tramo backward!\"\n",
    "\n",
    "    # Identificación de muestras problemáticas\n",
    "    error_counts_0, error_counts_1, oob_counts = bootstrap_oob_identification(\n",
    "        ds_train, \n",
    "        y_train,\n",
    "        n_models=hp['n_meta_learners']\n",
    "    )\n",
    "    best_fraction, best_score = optimize_bad_samples_threshold(\n",
    "        error_counts_0, \n",
    "        error_counts_1, \n",
    "        oob_counts\n",
    "    )\n",
    "    # Marcar muestras problemáticas con el mejor umbral\n",
    "    to_mark_0 = (error_counts_0 / oob_counts.replace(0, 1)).fillna(0)\n",
    "    to_mark_1 = (error_counts_1 / oob_counts.replace(0, 1)).fillna(0)\n",
    "    threshold_0 = np.percentile(to_mark_0[to_mark_0 > 0], 75) * best_fraction if len(to_mark_0[to_mark_0 > 0]) else 0\n",
    "    threshold_1 = np.percentile(to_mark_1[to_mark_1 > 0], 75) * best_fraction if len(to_mark_1[to_mark_1 > 0]) else 0\n",
    "    marked_0 = to_mark_0[to_mark_0 > threshold_0].index\n",
    "    marked_1 = to_mark_1[to_mark_1 > threshold_1].index\n",
    "    all_bad = pd.Index(marked_0).union(marked_1)\n",
    "    # Crear etiquetas meta\n",
    "    ds_train['meta_labels'] = 1.0\n",
    "    ds_train.loc[ds_train.index.isin(all_bad), 'meta_labels'] = 0.0\n",
    "\n",
    "    # Entrenar modelo principal solo con muestras buenas\n",
    "    good_samples = ds_train[ds_train['meta_labels'] == 1.0]\n",
    "    X_main = good_samples[feature_cols]\n",
    "    y_main = y_train[good_samples.index]\n",
    "    X_train, X_val, y_train_split, y_val = train_test_split(\n",
    "        X_main, y_main, train_size=0.8, test_size=0.2, shuffle=True)\n",
    "    cat_main_params = dict(\n",
    "        iterations=hp.get('cat_main_iterations', 500),\n",
    "        depth=hp.get('cat_main_depth', 6),\n",
    "        learning_rate=hp.get('cat_main_learning_rate', 0.15),\n",
    "        l2_leaf_reg=hp.get('cat_main_l2_leaf_reg', 3.0),\n",
    "        early_stopping_rounds=hp.get('cat_main_early_stopping'),\n",
    "        #rsm=hp.get('cat_main_rsm', 0.5),\n",
    "        custom_loss=['Accuracy'],\n",
    "        eval_metric='Accuracy',\n",
    "        use_best_model=True,\n",
    "        verbose=False,\n",
    "        thread_count=-1,\n",
    "        task_type='CPU',\n",
    "    )\n",
    "    main_model = CatBoostClassifier(**cat_main_params)\n",
    "    main_model.fit(X_train, y_train_split, eval_set=(X_val, y_val), plot=False)\n",
    "\n",
    "    # Entrenar modelo meta con todos los datos\n",
    "    X_meta = ds_train[feature_cols]\n",
    "    y_meta = ds_train['meta_labels']\n",
    "    X_meta_train, X_meta_val, y_meta_train, y_meta_val = train_test_split(\n",
    "        X_meta, y_meta, train_size=0.8, test_size=0.2, shuffle=True)\n",
    "    cat_meta_params = dict(\n",
    "        iterations=hp.get('cat_meta_iterations', 500),\n",
    "        depth=hp.get('cat_meta_depth', 6),\n",
    "        learning_rate=hp.get('cat_meta_learning_rate', 0.15),\n",
    "        l2_leaf_reg=hp.get('cat_meta_l2_leaf_reg', 3.0),\n",
    "        early_stopping_rounds=hp.get('cat_meta_early_stopping'),\n",
    "        custom_loss=['F1'],\n",
    "        eval_metric='F1',\n",
    "        use_best_model=True,\n",
    "        verbose=False,\n",
    "        thread_count=-1,\n",
    "        task_type='CPU',\n",
    "    )\n",
    "    meta_model = CatBoostClassifier(**cat_meta_params)\n",
    "    meta_model.fit(X_meta_train, y_meta_train, eval_set=(X_meta_val, y_meta_val), plot=False)\n",
    "\n",
    "    # Evaluar en datos fuera de muestra\n",
    "    ds_oos['close'] = full_ds.loc[oos_mask, 'close']\n",
    "    R2_forward = test_model_one_direction(ds_oos,\n",
    "                                         [main_model, meta_model],\n",
    "                                         hp['full_forward'],\n",
    "                                         hp['backward'],\n",
    "                                         hp['markup'],\n",
    "                                         hp['direction'],\n",
    "                                         plt=False)\n",
    "\n",
    "    # Evaluar en datos históricos\n",
    "    ds_train_val['close'] = full_ds.loc[ds_train.iloc[-len(ds_oos):].index, 'close']\n",
    "    R2_backward = test_model_one_direction(ds_train_val,\n",
    "                                         [main_model, meta_model],\n",
    "                                         hp['forward'],\n",
    "                                         hp['backward'],\n",
    "                                         hp['markup'],\n",
    "                                         hp['direction'],\n",
    "                                         plt=False)\n",
    "\n",
    "    if R2_forward > 0.0  and R2_backward > 0.0:\n",
    "        score = calc_score(R2_forward, R2_backward)\n",
    "    else:\n",
    "        score = -10.0\n",
    "    \n",
    "    if score > best_combined_score:\n",
    "        best_combined_score = score\n",
    "        # Guardar información del trial actual\n",
    "        trial.set_user_attr(\"forward_r2\", R2_forward)\n",
    "        trial.set_user_attr(\"backward_r2\", R2_backward)\n",
    "        trial.set_user_attr(\"combined_score\", score)\n",
    "        trial.set_user_attr(\"stats_main\", hp[\"stats_main\"])\n",
    "        trial.set_user_attr(\"periods_main\", hp[\"periods_main\"])\n",
    "        trial.set_user_attr(\"n_meta_learners\", hp[\"n_meta_learners\"])\n",
    "        trial.set_user_attr(\"bad_samples_fraction\", hp[\"bad_samples_fraction\"])\n",
    "        \n",
    "        # Guardar parámetros del trial actual (sin fechas)\n",
    "        params_to_save = hp.copy()\n",
    "        params_to_save.pop('backward', None)\n",
    "        params_to_save.pop('forward', None)\n",
    "        params_to_save.pop('full_forward', None)\n",
    "        trial.set_user_attr(\"params\", params_to_save)\n",
    "        \n",
    "        # Si existe el estudio, actualizar sus atributos\n",
    "        if study is not None:\n",
    "            current_best = study.user_attrs.get(\"best_combined_score\", -math.inf)\n",
    "            if score > current_best:\n",
    "                study.set_user_attr(\"best_params\", params_to_save)\n",
    "                study.set_user_attr(\"best_metrics\", {\n",
    "                    \"forward_r2\": R2_forward,\n",
    "                    \"backward_r2\": R2_backward,\n",
    "                    \"combined_score\": score,\n",
    "                })\n",
    "                study.set_user_attr(\"best_combined_score\", score)\n",
    "                study.set_user_attr(\"best_models\", [main_model, meta_model])\n",
    "                study.set_user_attr(\"best_stats\", hp[\"stats_main\"])\n",
    "                study.set_user_attr(\"best_periods\", hp[\"periods_main\"])\n",
    "                study.set_user_attr(\"best_trial_number\", trial.number)\n",
    "                study.set_user_attr(\"best_df_sample\", full_ds.sample(1))\n",
    "\n",
    "    return score\n",
    "\n",
    "def optimize_and_export(base_hp: Dict[str, Any]):\n",
    "    \"\"\"Lanza Optuna, guarda el mejor modelo y lo exporta a ONNX.\"\"\"\n",
    "    try:\n",
    "        # Crear el estudio\n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            pruner=HyperbandPruner(),\n",
    "            sampler=optuna.samplers.TPESampler(\n",
    "                n_startup_trials=int(np.sqrt(base_hp['n_trials'])),\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        study.optimize(lambda t: objective(t, base_hp, study),\n",
    "                    n_trials=base_hp['n_trials'],\n",
    "                    show_progress_bar=True)\n",
    "\n",
    "        # Obtener el mejor modelo\n",
    "        best_models = study.user_attrs.get(\"best_models\")\n",
    "        if not (best_models and len(best_models) == 2 and all(best_models)):\n",
    "            print(\"⚠️  Error: best_models incorrecto\")\n",
    "            return None\n",
    "\n",
    "        # Exportar modelos CatBoost directamente\n",
    "        export_params = base_hp.copy()\n",
    "        export_params.update({\n",
    "            \"best_trial\": study.user_attrs[\"best_trial_number\"],\n",
    "            \"best_score\": study.user_attrs[\"best_combined_score\"],\n",
    "            \"best_periods\": study.user_attrs[\"best_periods\"],\n",
    "            \"best_stats\": study.user_attrs[\"best_stats\"],\n",
    "            \"best_stats_meta\": study.user_attrs[\"best_stats_meta\"],\n",
    "            \"best_models\": best_models,  # Los modelos CatBoost directamente\n",
    "        })\n",
    "        \n",
    "        export_model_to_ONNX(**export_params)\n",
    "        \n",
    "        return {\n",
    "            \"forward_r2\": study.user_attrs.get(\"best_metrics\", {}).get(\"forward_r2\", float(\"nan\")),\n",
    "            \"backward_r2\": study.user_attrs.get(\"best_metrics\", {}).get(\"backward_r2\", float(\"nan\")),\n",
    "            \"combined_score\": study.user_attrs.get(\"best_metrics\", {}).get(\"combined_score\", float(\"nan\")),\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error en optimize_and_export: {str(e)}\")\n",
    "        import traceback\n",
    "        print(traceback.format_exc())\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_hp: Dict[str, Any] = {\n",
    "        'symbol': r'XAUUSD',\n",
    "        'timeframe': 'H1',\n",
    "        'direction': 'buy',\n",
    "        'backward': datetime(2020, 2, 1),\n",
    "        'forward': datetime(2024, 2, 1),\n",
    "        'full_forward': datetime(2026, 2, 1),\n",
    "        'model_seed': 0,\n",
    "        'n_trials': 1000,\n",
    "        'models_export_path': r'/mnt/c/Users/Administrador/AppData/Roaming/MetaQuotes/Terminal/6C3C6A11D1C3791DD4DBF45421BF8028/MQL5/Files/',\n",
    "        'include_export_path': r'/mnt/c/Users/Administrador/AppData/Roaming/MetaQuotes/Terminal/6C3C6A11D1C3791DD4DBF45421BF8028/MQL5/Include/ajmtrz/include/Dmitrievsky',\n",
    "        'history_path': r\"/mnt/c/Users/Administrador/AppData/Roaming/MetaQuotes/Terminal/Common/Files/\",\n",
    "        'stats_main': [],\n",
    "        'best_models': [None, None],\n",
    "        'markup': 0.20,\n",
    "        'label_min'  : 1,\n",
    "        'label_max'  : 15,\n",
    "        'window_size': 350,\n",
    "        'periods_main': [i for i in range(5, 300, 30)],\n",
    "    }\n",
    "    \n",
    "    base_df = get_prices(base_hp)\n",
    "    base_hp.update({\n",
    "        'base_df': base_df,\n",
    "    })\n",
    "    # Para recopilar resultados globales de todos los modelos\n",
    "    all_results = {}\n",
    "    best_models = []\n",
    "    model_range = range(0, 10)\n",
    "    base_hp.update({\n",
    "        'n_trials': 500,\n",
    "    })\n",
    "    for i in tqdm(model_range, desc=f\"Optimizando {base_hp['symbol']}/{base_hp['timeframe']}\", unit=\"modelo\"):\n",
    "        try:\n",
    "            model_results = optimize_and_export(base_hp)\n",
    "            best_models.append((i, model_results))\n",
    "            \n",
    "            # Añadir a resultados globales\n",
    "            all_results[f\"model_{i}\"] = {\n",
    "                \"success\": True,\n",
    "                \"forward_r2\": model_results[\"forward_r2\"],\n",
    "                \"backward_r2\": model_results[\"backward_r2\"],\n",
    "                \"combined_score\": model_results[\"combined_score\"]\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            tqdm.write(f\"\\nError procesando modelo {i}: {str(e)}\")\n",
    "            tqdm.write(traceback.format_exc())\n",
    "            \n",
    "            all_results[f\"model_{i}\"] = {\n",
    "                \"success\": False,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "            continue\n",
    "    \n",
    "    # Resumen final\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"RESUMEN DE OPTIMIZACIÓN {base_hp['symbol']}/{base_hp['timeframe']}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    successful_models = [info for model_key, info  in all_results.items() if info.get(\"success\", False)]\n",
    "    print(f\"Modelos completados exitosamente: {len(successful_models)}/{len(model_range)}\")\n",
    "\n",
    "    if successful_models:\n",
    "        # Calcular estadísticas globales\n",
    "        forward_scores = [info[\"forward_r2\"] for info in successful_models]\n",
    "        backward_scores = [info[\"backward_r2\"] for info in successful_models]\n",
    "        combined_scores = [info[\"combined_score\"] for info in successful_models]\n",
    "        \n",
    "        print(f\"\\nEstadísticas de rendimiento:\")\n",
    "        print(f\"  Forward R2 promedio: {np.mean(forward_scores):.4f} ± {np.std(forward_scores):.4f}\")\n",
    "        print(f\"  Backward R2 promedio: {np.mean(backward_scores):.4f} ± {np.std(backward_scores):.4f}\")\n",
    "        print(f\"  Puntuación combinada promedio: {np.mean(combined_scores):.4f} ± {np.std(combined_scores):.4f}\")\n",
    "\n",
    "        # Identificar el mejor modelo global basado en la puntuación combinada\n",
    "        successful = [(k, v) for k, v in all_results.items() if v.get(\"success\", False)]\n",
    "        combined_scores = [v[\"combined_score\"] for _, v in successful]\n",
    "        best_model_key, best_info = successful[int(np.argmax(combined_scores))]\n",
    "        \n",
    "        print(f\"\\nMejor modelo global: {best_model_key}\")\n",
    "        print(f\"  Forward R2: {best_info['forward_r2']:.4f}\")\n",
    "        print(f\"  Backward R2: {best_info['backward_r2']:.4f}\")\n",
    "        print(f\"  Puntuación combinada: {best_info['combined_score']:.4f}\")\n",
    "    \n",
    "    print(\"\\nProceso de optimización completado.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
