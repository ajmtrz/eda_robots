{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5302a1a",
   "metadata": {},
   "source": [
    "## Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eed4c641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from numba import njit\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any, Tuple, List\n",
    "import optuna\n",
    "from optuna.pruners import SuccessiveHalvingPruner, HyperbandPruner\n",
    "from optuna.storages import RDBStorage\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from modules.labeling_lib import get_prices\n",
    "from modules.labeling_lib import get_clustering_features\n",
    "from modules.labeling_lib import get_labels_one_direction\n",
    "from modules.labeling_lib import sliding_window_clustering\n",
    "from modules.tester_lib import test_model_one_direction_clustering\n",
    "from modules.export_lib import export_model_to_ONNX\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15736753",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e880b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_final_models(clustered: pd.DataFrame,\n",
    "                     meta: pd.DataFrame,\n",
    "                     oos_data: pd.DataFrame,\n",
    "                     hp: Dict[str, Any]) -> Tuple[float, Any, Any]:\n",
    "    \"\"\"Entrena modelo principal + meta‑modelo y evalúa en OOS.\n",
    "\n",
    "    Devuelve (R2, model, meta_model).\n",
    "    \"\"\"\n",
    "    # ---------- 1) main model ----------\n",
    "    X_main = clustered.drop(columns=['labels', *meta.columns[meta.columns.str.contains('_meta_feature')]])\n",
    "    y_main = clustered['labels'].astype('int16')\n",
    "\n",
    "    # ---------- 2) meta‑model ----------\n",
    "    X_meta = meta.loc[:, meta.columns.str.contains('_meta_feature')]\n",
    "    y_meta = meta['clusters'].astype('int16')\n",
    "    # 3) Split aleatorio (70/30)\n",
    "    train_X, test_X, train_y, test_y = train_test_split(\n",
    "        X_main, y_main, train_size=0.7, shuffle=True)\n",
    "    train_X_m, test_X_m, train_y_m, test_y_m = train_test_split(\n",
    "        X_meta, y_meta, train_size=0.7, shuffle=True)\n",
    "    # debug\n",
    "    # common_index = X_main.index[0]\n",
    "    # display(X_main.loc[[common_index]])\n",
    "    # display(X_meta.loc[[common_index]])\n",
    "    # 4) Hiper‑parámetros CatBoost (con valores por defecto + overrides)\n",
    "    cat_main_params = dict(\n",
    "        iterations=hp.get('cat_main_iterations', 500),\n",
    "        depth=hp.get('cat_main_depth', 6),\n",
    "        learning_rate=hp.get('cat_main_learning_rate', 0.15),\n",
    "        l2_leaf_reg=hp.get('cat_main_l2_leaf_reg', 3.0),\n",
    "        custom_loss=['Accuracy'],\n",
    "        eval_metric='Accuracy',\n",
    "        use_best_model=True,\n",
    "        verbose=False,\n",
    "        thread_count=-1,\n",
    "        task_type='CPU',\n",
    "    )\n",
    "    model = CatBoostClassifier(**cat_main_params)\n",
    "    model.fit(train_X, train_y, eval_set=(test_X, test_y), early_stopping_rounds=25)\n",
    "\n",
    "    cat_meta_params = dict(\n",
    "        iterations=hp.get('cat_meta_iterations', 500),\n",
    "        depth=hp.get('cat_meta_depth', 6),\n",
    "        learning_rate=hp.get('cat_meta_learning_rate', 0.15),\n",
    "        l2_leaf_reg=hp.get('cat_meta_l2_leaf_reg', 3.0),\n",
    "        custom_loss=['F1'],\n",
    "        eval_metric='F1',\n",
    "        use_best_model=True,\n",
    "        verbose=False,\n",
    "        thread_count=-1,\n",
    "        task_type='CPU',\n",
    "    )\n",
    "    meta_model = CatBoostClassifier(**cat_meta_params)\n",
    "    meta_model.fit(train_X_m, train_y_m, eval_set=(test_X_m, test_y_m), early_stopping_rounds=15)\n",
    "\n",
    "    # 5) Evaluación en datos fuera de muestra\n",
    "    R2 = test_model_one_direction_clustering(\n",
    "        oos_data,\n",
    "        [model, meta_model],\n",
    "        hp['full forward'],\n",
    "        hp['forward'],\n",
    "        hp['markup'],\n",
    "        hp['direction'],\n",
    "        plt=False,\n",
    "    )\n",
    "    if math.isnan(R2):\n",
    "        R2 = -1.0\n",
    "    return R2, model, meta_model\n",
    "\n",
    "def objective(trial: optuna.trial.Trial, base_hp: Dict[str, Any], study=None) -> float:\n",
    "    hp = base_hp.copy()\n",
    "\n",
    "    # µ··· Espacio de búsqueda optimizado ···µ\n",
    "    # Parámetros de clustering más amplios para encontrar patrones más diversos\n",
    "    hp['n_clusters'] = trial.suggest_int('n_clusters', 5, 50, step=5)  \n",
    "    hp['window_size'] = trial.suggest_int('window_size', 50, 500, step=10)\n",
    "    \n",
    "    # Parámetros de etiquetado más agresivos\n",
    "    hp['label_min'] = trial.suggest_int('label_min', 1, 5)\n",
    "    hp['label_max'] = trial.suggest_int('label_max', hp['label_min']+5, 30)\n",
    "    hp['markup'] = trial.suggest_float(\"markup\", 0.1, 0.4)\n",
    "\n",
    "    # CatBoost principal - Mayor capacidad de aprendizaje\n",
    "    hp['cat_main_iterations'] = trial.suggest_int('cat_main_iterations', 300, 2000, step=100)\n",
    "    hp['cat_main_depth'] = trial.suggest_int('cat_main_depth', 6, 12)\n",
    "    hp['cat_main_learning_rate'] = trial.suggest_float('cat_main_learning_rate', 0.005, 0.4, log=True)\n",
    "    hp['cat_main_l2_leaf_reg'] = trial.suggest_float('cat_main_l2_leaf_reg', 0.5, 10.0)\n",
    "\n",
    "    # CatBoost meta - Enfoque en precisión\n",
    "    hp['cat_meta_iterations'] = trial.suggest_int('cat_meta_iterations', 200, 1000, step=100)\n",
    "    hp['cat_meta_depth'] = trial.suggest_int('cat_meta_depth', 5, 10)\n",
    "    hp['cat_meta_learning_rate'] = trial.suggest_float('cat_meta_learning_rate', 0.01, 0.3, log=True)\n",
    "    hp['cat_meta_l2_leaf_reg'] = trial.suggest_float('cat_meta_l2_leaf_reg', 0.5, 8.0)\n",
    "\n",
    "    # Optimización de períodos para el modelo principal\n",
    "    n_periods_main = trial.suggest_int('n_periods_main', 5, 15)\n",
    "    main_periods = []\n",
    "    for i in range(n_periods_main):\n",
    "        period_main = trial.suggest_int(f'period_main_{i}', 5, 200, log=True)\n",
    "        main_periods.append(period_main)\n",
    "    main_periods = sorted(list(set(main_periods)))  # Eliminar duplicados y ordenar\n",
    "    if len(main_periods) < 3:  # Asegurar mínimo de períodos\n",
    "        return -np.inf\n",
    "    hp['periods_main'] = main_periods\n",
    "\n",
    "    # Optimización de períodos para el meta-modelo\n",
    "    n_periods_meta = 1 #trial.suggest_int('n_periods_meta', 1, 2)\n",
    "    meta_periods = []\n",
    "    for i in range(n_periods_meta):\n",
    "        period_meta = trial.suggest_int(f'period_meta_{i}', 3, 5)\n",
    "        meta_periods.append(period_meta)\n",
    "    meta_periods = sorted(list(set(meta_periods)))  # Eliminar duplicados y ordenar\n",
    "    hp['periods_meta'] = meta_periods\n",
    "\n",
    "    # Selección de estadísticas para el modelo principal\n",
    "    main_stat_choices = [\n",
    "        \"std\", \"skew\", \"kurt\", \"zscore\", \"mean\", \"range\", \"median\", \n",
    "        \"mad\", \"var\", \"entropy\", \"slope\", \"momentum\", \"roc\", \"fractal\", \"hurst\"\n",
    "    ]\n",
    "    n_main_stats = trial.suggest_int('n_main_stats', 1, 5)\n",
    "    selected_main_stats = []\n",
    "    for i in range(n_main_stats):\n",
    "        stat = trial.suggest_categorical(f'main_stat_{i}', main_stat_choices)\n",
    "        selected_main_stats.append(stat)\n",
    "    selected_main_stats = list(set(selected_main_stats))\n",
    "    if len(selected_main_stats) == 1 and \"fractal\" in selected_main_stats:\n",
    "        remaining_stats = [s for s in main_stat_choices if s != \"fractal\"]\n",
    "        additional_stat = trial.suggest_categorical('additional_stat', remaining_stats)\n",
    "        selected_main_stats.append(additional_stat)\n",
    "    hp[\"stats_main\"] = selected_main_stats\n",
    "    #print(f\"Main features seleccionadas: {hp['stats_main']}\")\n",
    "\n",
    "    # Selección de estadísticas para el meta-modelo\n",
    "    meta_stat_choices = [\n",
    "        \"std\", \"skew\", \"zscore\", \"range\", \"mad\", \n",
    "        \"var\", \"entropy\", \"slope\", \"momentum\", \"roc\"\n",
    "    ]\n",
    "    # Seleccionar una única estadística meta\n",
    "    selected_meta_stat = trial.suggest_categorical('meta_stat', meta_stat_choices)\n",
    "    hp[\"stats_meta\"] = [selected_meta_stat]\n",
    "    #print(f\"Meta features seleccionadas: {hp['stats_meta']}\")\n",
    "\n",
    "    # Dataset completo\n",
    "    full_ds = get_clustering_features(get_prices(hp), hp)\n",
    "    ds_train = full_ds[(full_ds.index > hp['backward']) & (full_ds.index < hp['forward'])]\n",
    "    ds_oos = full_ds[(full_ds.index >= hp['forward']) & (full_ds.index < hp['full forward'])]\n",
    "    \n",
    "    # Clustering con ventana deslizante\n",
    "    data = sliding_window_clustering(\n",
    "        ds_train,\n",
    "        n_clusters=hp['n_clusters'],\n",
    "        window_size=hp['window_size']\n",
    "    )\n",
    "    \n",
    "    best_R2 = -math.inf\n",
    "    valid_clusters = 0\n",
    "    \n",
    "    # Evaluar clusters ordenados por tamaño\n",
    "    cluster_sizes = data['clusters'].value_counts()\n",
    "    for clust in cluster_sizes.index:\n",
    "        clustered_data = data[data['clusters'] == clust].copy()\n",
    "        if len(clustered_data) < 750:  # Aumentar mínimo de muestras\n",
    "            continue\n",
    "            \n",
    "        valid_clusters += 1\n",
    "        clustered_data = get_labels_one_direction(\n",
    "            clustered_data,\n",
    "            markup=hp['markup'],\n",
    "            min=hp['label_min'],\n",
    "            max=hp['label_max'],\n",
    "            direction=hp['direction'])\n",
    "\n",
    "        clustered_data = clustered_data.drop(['close', 'clusters'], axis=1)\n",
    "        meta_data = data.copy()\n",
    "        meta_data['clusters'] = (meta_data['clusters'] == clust).astype(int)\n",
    "\n",
    "        R2, model, meta_model = fit_final_models(\n",
    "            clustered_data,\n",
    "            meta_data.drop(['close'], axis=1),\n",
    "            ds_oos,\n",
    "            hp\n",
    "        )\n",
    "\n",
    "        # Al final de la función objective, modifica la parte donde guarda el mejor resultado:\n",
    "        if R2 > best_R2:\n",
    "            best_R2 = R2\n",
    "            best_pack = (model, meta_model)\n",
    "            \n",
    "            if study is not None:\n",
    "                prev_best = study.user_attrs.get(\"best_r2\", -np.inf)\n",
    "                if best_R2 > prev_best:\n",
    "                    study.set_user_attr(\"best_model\", best_pack[0])\n",
    "                    study.set_user_attr(\"best_meta_model\", best_pack[1])\n",
    "                    study.set_user_attr(\"best_r2\", best_R2)\n",
    "                    study.set_user_attr(\"best_stats_main\", hp[\"stats_main\"])\n",
    "                    study.set_user_attr(\"best_stats_meta\", hp[\"stats_meta\"])\n",
    "                    study.set_user_attr(\"best_periods\", hp[\"periods_main\"])\n",
    "                    study.set_user_attr(\"best_periods_meta\", hp[\"periods_meta\"])\n",
    "                    \n",
    "    # Penalizar si muy pocos clusters válidos\n",
    "    if valid_clusters < 3:\n",
    "        best_R2 *= 0.5\n",
    "\n",
    "    return best_R2\n",
    "\n",
    "def optimize_and_export(symbol, timeframe, model_number, n_trials):\n",
    "    \"\"\"Lanza Optuna, guarda el mejor modelo y lo exporta a ONNX.\"\"\"\n",
    "\n",
    "    common_file_folder = r\"/mnt/c/Users/Administrador/AppData/Roaming/MetaQuotes/Terminal/Common/Files/\"\n",
    "    mql5_files_folder = r'/mnt/c/Users/Administrador/AppData/Roaming/MetaQuotes/Terminal/6C3C6A11D1C3791DD4DBF45421BF8028/MQL5/Files/'\n",
    "    mql5_include_folder = r'/mnt/c/Users/Administrador/AppData/Roaming/MetaQuotes/Terminal/6C3C6A11D1C3791DD4DBF45421BF8028/MQL5/Include/ajmtrz/include/Dmitrievsky'\n",
    "\n",
    "    # Crear directorio para la base de datos si no existe\n",
    "    db_dir = os.path.join(mql5_files_folder, 'optuna_db')\n",
    "    os.makedirs(db_dir, exist_ok=True)\n",
    "    db_path = os.path.join(db_dir, f'{symbol}_{timeframe}_study.db')\n",
    "\n",
    "    # Configurar el pruner inteligente\n",
    "    pruner = SuccessiveHalvingPruner(\n",
    "        min_resource=1,\n",
    "        reduction_factor=3,\n",
    "        min_early_stopping_rate=0\n",
    "    )\n",
    "\n",
    "    # Crear el estudio con persistencia\n",
    "    storage = RDBStorage(f\"sqlite:///{db_path}\")\n",
    "    study = optuna.create_study(\n",
    "        direction='maximize',\n",
    "        pruner=pruner,\n",
    "        storage=storage,\n",
    "        load_if_exists=True,\n",
    "        study_name=f\"{symbol}_{timeframe}_model{model_number}_study\"\n",
    "    )\n",
    "\n",
    "    base_hp: Dict[str, Any] = {\n",
    "        'symbol': symbol,\n",
    "        'timeframe': timeframe,\n",
    "        'models_export_path': mql5_files_folder,\n",
    "        'include_export_path': mql5_include_folder,\n",
    "        'history_path': common_file_folder,\n",
    "        'best_models': [],\n",
    "        'stats_main': [],\n",
    "        'stats_meta': [\"std\"],  # Por defecto usa std\n",
    "        'model_number': model_number,\n",
    "        'markup': 0.20,\n",
    "        'label_min'  : 1,\n",
    "        'label_max'  : 15,\n",
    "        'direction': 'buy',\n",
    "        'n_clusters': 30,\n",
    "        'window_size': 350,\n",
    "        'periods_main': [i for i in range(5, 300, 30)],\n",
    "        'periods_meta': [5],\n",
    "        'backward': datetime(2020, 3, 26),\n",
    "        'forward': datetime(2024, 1, 1),\n",
    "        'full forward': datetime(2026, 1, 1),\n",
    "    }\n",
    "\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(lambda t: objective(t, base_hp, study), n_trials=n_trials, show_progress_bar=False)\n",
    "\n",
    "    print(\"\\n┌───────────────────────────────────────────────┐\")\n",
    "    print(f\"│      MEJOR RESULTADO {model_number} = {study.best_value:.4f}│\")\n",
    "    print(\"└───────────────────────────────────────────────┘\\n\")\n",
    "    print(\"Parámetros óptimos:\\n\", study.best_params)\n",
    "\n",
    "    # Recuperar el mejor modelo y meta‑modelo\n",
    "    base_hp.update(study.best_params)\n",
    "    model      = study.user_attrs[\"best_model\"]\n",
    "    meta_model = study.user_attrs[\"best_meta_model\"]\n",
    "    best_r2    = study.user_attrs[\"best_r2\"]\n",
    "    base_hp['stats_main'] = study.user_attrs[\"best_stats_main\"]\n",
    "    base_hp['stats_meta'] = study.user_attrs[\"best_stats_meta\"]\n",
    "    base_hp['periods_main'] = study.user_attrs[\"best_periods\"]\n",
    "    base_hp['periods_meta'] = study.user_attrs[\"best_periods_meta\"]\n",
    "    base_hp.pop('best_models', None)\n",
    "    print(\"Exportando modelos ONNX… R2 = {:.4f}\".format(best_r2))\n",
    "    export_model_to_ONNX(best_models=[model, meta_model], **base_hp)\n",
    "\n",
    "def create_study_with_pruner(symbol, timeframe):\n",
    "    pruner = SuccessiveHalvingPruner(\n",
    "        min_resource=1,\n",
    "        reduction_factor=3,\n",
    "        min_early_stopping_rate=0\n",
    "    )\n",
    "    \n",
    "    study = optuna.create_study(\n",
    "        direction='maximize',\n",
    "        pruner=pruner,\n",
    "        storage='sqlite:///optuna_study.db',  # Persistencia\n",
    "        load_if_exists=True,  # Cargar estudios previos\n",
    "        study_name=f\"{symbol}_{timeframe}_study\"\n",
    "    )\n",
    "    return study\n",
    "\n",
    "def cleanup_old_studies(db_path, max_studies=10):\n",
    "    \"\"\"Limpia estudios antiguos manteniendo solo los más recientes.\"\"\"\n",
    "    try:\n",
    "        storage = RDBStorage(f\"sqlite:///{db_path}\")\n",
    "        all_studies = storage.get_all_studies()\n",
    "        \n",
    "        if len(all_studies) > max_studies:\n",
    "            # Ordenar estudios por fecha de creación\n",
    "            sorted_studies = sorted(all_studies, key=lambda x: x.datetime_start)\n",
    "            # Eliminar los más antiguos\n",
    "            for study in sorted_studies[:-max_studies]:\n",
    "                storage.delete_study(study.study_id)\n",
    "    except Exception as e:\n",
    "        print(f\"Error al limpiar estudios antiguos: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    symbol = 'XAUUSD'\n",
    "    timeframe = 'H1'\n",
    "    n_trials_per_model = 50\n",
    "    model_range = range(0, 5)\n",
    "    \n",
    "    # Configurar ruta de la base de datos\n",
    "    db_dir = os.path.join(r'/mnt/c/Users/Administrador/AppData/Roaming/MetaQuotes/Terminal/6C3C6A11D1C3791DD4DBF45421BF8028/MQL5/Files/', 'optuna_db')\n",
    "    db_path = os.path.join(db_dir, f'{symbol}_{timeframe}_study.db')\n",
    "    \n",
    "    # Limpiar estudios antiguos antes de comenzar\n",
    "    cleanup_old_studies(db_path, max_studies=20)\n",
    "    \n",
    "    # Para recopilar resultados globales de todos los modelos\n",
    "    all_results = {}\n",
    "    best_models = []\n",
    "    \n",
    "    for i in tqdm(model_range, desc=f\"Optimizando {symbol}/{timeframe}\", unit=\"modelo\"):\n",
    "        try:\n",
    "            model_params = optimize_and_export(symbol, timeframe, i, n_trials=n_trials_per_model)\n",
    "            best_models.append((i, model_params))\n",
    "            \n",
    "            # Añadir a resultados globales\n",
    "            all_results[f\"model_{i}\"] = {\n",
    "                \"success\": True,\n",
    "                \"model_saved\": model_params is not None\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            tqdm.write(f\"\\nError procesando modelo {i}: {str(e)}\")\n",
    "            tqdm.write(traceback.format_exc())\n",
    "            \n",
    "            all_results[f\"model_{i}\"] = {\n",
    "                \"success\": False,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "            continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
