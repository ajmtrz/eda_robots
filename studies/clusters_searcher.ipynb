{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15736753",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e880b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizando XAUUSD/H1:   0%|          | 0/5 [00:00<?, ?modelo/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00657e76aaf149e5ac1beef61f96c1c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gc\n",
    "import copy\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from typing import Dict, Any, Tuple\n",
    "import optuna\n",
    "from optuna.pruners import HyperbandPruner#, SuccessiveHalvingPruner\n",
    "from optuna.integration import CatBoostPruningCallback\n",
    "from optuna.integration import XGBoostPruningCallback\n",
    "from optuna.integration import LightGBMPruningCallback\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import set_config\n",
    "from modules.labeling_lib import get_prices\n",
    "from modules.labeling_lib import get_features\n",
    "from modules.labeling_lib import get_labels_one_direction\n",
    "from modules.labeling_lib import sliding_window_clustering\n",
    "from modules.tester_lib import robust_oos_score_one_direction\n",
    "from modules.tester_lib import walk_forward_score_one_direction\n",
    "from modules.export_lib import export_model_to_ONNX\n",
    "from modules.export_lib import XGBWithEval\n",
    "from modules.export_lib import LGBMWithEval\n",
    "from modules.export_lib import CatWithEval\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "set_config(enable_metadata_routing=True, skip_parameter_validation=True)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "def fit_final_models(main_data: pd.DataFrame,\n",
    "                    meta_data: pd.DataFrame,\n",
    "                    ds_test: pd.DataFrame,\n",
    "                    ds_train: pd.DataFrame,\n",
    "                    hp: Dict[str, Any],\n",
    "                    trial: optuna.trial.Trial) -> Tuple[float, float, Any, Any]:\n",
    "    def check_constant_features(X):\n",
    "        return np.any(np.var(X, axis=0) < 1e-10)\n",
    "    \n",
    "    gc.collect()\n",
    "    # ---------- 1) main model_main ----------\n",
    "    X_main = main_data.loc[:, main_data.columns.str.contains('_feature') & ~main_data.columns.str.contains('_meta_feature')]\n",
    "    if X_main.shape[1] == 1:\n",
    "        if check_constant_features(X_main.to_numpy()):\n",
    "            return None, None, None, None\n",
    "    y_main = main_data['labels'].astype('int16')\n",
    "    # Check for inf values in main features\n",
    "    inf_cols_main = X_main.columns[X_main.isin([np.inf, -np.inf]).any()].tolist()\n",
    "    if inf_cols_main:\n",
    "        print(\"Main features with inf values:\", inf_cols_main)\n",
    "    # Check for NaN values in main features\n",
    "    nan_cols_main = X_main.columns[X_main.isna().any()].tolist()\n",
    "    if nan_cols_main:\n",
    "        print(\"Main features with NaN values:\", nan_cols_main)\n",
    "    # División de datos para el modelo principal según fechas\n",
    "    X_train_main, X_val_main, y_train_main, y_val_main = train_test_split(\n",
    "        X_main, y_main, \n",
    "        test_size=0.2,\n",
    "        shuffle=True\n",
    "    )\n",
    "    # ── descartar clusters problemáticos ────────────────────────────\n",
    "    if len(y_train_main.value_counts()) < 2 or len(y_val_main.value_counts()) < 2:\n",
    "        return None, None, None, None\n",
    "    \n",
    "    # ---------- 2) meta‑modelo ----------\n",
    "    X_meta = meta_data.loc[:, meta_data.columns.str.contains('_meta_feature')]\n",
    "    if X_meta.shape[1] == 1:\n",
    "        if check_constant_features(X_meta.to_numpy()):\n",
    "            return None, None, None, None\n",
    "    y_meta = meta_data['clusters'].astype('int16')\n",
    "    # Check for inf values in meta features\n",
    "    inf_cols_meta = X_meta.columns[X_meta.isin([np.inf, -np.inf]).any()].tolist()\n",
    "    if inf_cols_meta:\n",
    "        print(\"Meta features with inf values:\", inf_cols_meta)\n",
    "    # Check for NaN values in meta features\n",
    "    nan_cols_meta = X_meta.columns[X_meta.isna().any()].tolist()\n",
    "    if nan_cols_meta:\n",
    "        print(\"Meta features with NaN values:\", nan_cols_meta)\n",
    "    # División de datos para el modelo principal según fechas\n",
    "    X_train_meta, X_val_meta, y_train_meta, y_val_meta = train_test_split(\n",
    "        X_meta, y_meta, \n",
    "        test_size=0.2,\n",
    "        shuffle=True\n",
    "    )\n",
    "    # ── descartar clusters problemáticos ────────────────────────────\n",
    "    if len(y_train_meta.value_counts()) < 2 or len(y_val_meta.value_counts()) < 2:\n",
    "        return None, None, None, None\n",
    "\n",
    "    # print(f\"Main columns: {X_main.columns.tolist()}\")\n",
    "    # print(f\"Meta columns: {X_meta.columns.tolist()}\")\n",
    "    \n",
    "    # Main model\n",
    "    cat_main_params = dict(\n",
    "        iterations=hp['cat_main_iterations'],\n",
    "        depth=hp['cat_main_depth'],\n",
    "        learning_rate=hp['cat_main_learning_rate'],\n",
    "        l2_leaf_reg=hp['cat_main_l2_leaf_reg'],\n",
    "        early_stopping_rounds=hp['cat_main_early_stopping'],\n",
    "        eval_metric='Accuracy',\n",
    "        verbose=False,\n",
    "        thread_count=-1,\n",
    "        task_type='CPU',\n",
    "        used_ram_limit=\"16gb\"\n",
    "    )\n",
    "    xgb_main_params = dict(\n",
    "        n_estimators=hp['xgb_main_estimators'],\n",
    "        max_depth=hp['xgb_main_max_depth'],\n",
    "        learning_rate=hp['xgb_main_learning_rate'],\n",
    "        reg_lambda=hp['xgb_main_reg_lambda'],\n",
    "        early_stopping_rounds=hp['xgb_main_early_stopping'],\n",
    "        eval_metric='logloss',\n",
    "        verbosity=0,\n",
    "        n_jobs=-1,\n",
    "        tree_method= \"gpu_hist\",\n",
    "        device_type=\"cuda\"\n",
    "    )\n",
    "    lgbm_main_params = dict(\n",
    "        n_estimators=hp['lgbm_main_estimators'],\n",
    "        max_depth=hp['lgbm_main_max_depth'],\n",
    "        learning_rate=hp['lgbm_main_learning_rate'],\n",
    "        reg_lambda=hp['lgbm_main_reg_lambda'],\n",
    "        early_stopping_round=hp['lgbm_main_early_stopping'],\n",
    "        metric='auc',\n",
    "        tree_learner=\"serial\",\n",
    "        device=\"cpu\",\n",
    "        verbosity=-1,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    base_main_models = [\n",
    "        ('catboost', CatWithEval(\n",
    "            **cat_main_params,\n",
    "            eval_set=[(X_val_main, y_val_main)],\n",
    "            callbacks=[CatBoostPruningCallback(trial, \"Accuracy\")])),\n",
    "        ('xgboost', XGBWithEval(\n",
    "            **xgb_main_params, \n",
    "            eval_set=[(X_val_main, y_val_main)],\n",
    "            callbacks=[XGBoostPruningCallback(trial, \"validation_0-logloss\")]\n",
    "        )),\n",
    "        ('lightgbm', LGBMWithEval(\n",
    "            **lgbm_main_params, \n",
    "            eval_set=[(X_val_main, y_val_main)],\n",
    "            callbacks=[LightGBMPruningCallback(trial, \"auc\")])),\n",
    "    ]\n",
    "    model_main = VotingClassifier(\n",
    "            estimators=base_main_models,\n",
    "            voting='soft',\n",
    "            flatten_transform=False,\n",
    "            n_jobs=1\n",
    "        )\n",
    "    # print(\"training main model...\")\n",
    "    # start_time = time.time()\n",
    "    model_main.fit(X_train_main, y_train_main)\n",
    "    #print(f\"main model trained in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    # Meta-modelo\n",
    "    cat_meta_params = dict(\n",
    "        iterations=hp['cat_meta_iterations'],\n",
    "        depth=hp['cat_meta_depth'],\n",
    "        learning_rate=hp['cat_meta_learning_rate'],\n",
    "        l2_leaf_reg=hp['cat_meta_l2_leaf_reg'],\n",
    "        early_stopping_rounds=hp['cat_meta_early_stopping'],\n",
    "        eval_metric='F1',\n",
    "        verbose=False,\n",
    "        thread_count=-1,\n",
    "        task_type='CPU',\n",
    "        used_ram_limit=\"16gb\"\n",
    "    )\n",
    "    xgb_meta_params = dict(\n",
    "        n_estimators=hp['xgb_meta_estimators'],\n",
    "        max_depth=hp['xgb_meta_max_depth'],\n",
    "        learning_rate=hp['xgb_meta_learning_rate'],\n",
    "        reg_lambda=hp['xgb_meta_reg_lambda'],\n",
    "        early_stopping_rounds=hp['xgb_meta_early_stopping'],\n",
    "        eval_metric='auc',\n",
    "        verbosity=0,\n",
    "        verbose_eval=False,\n",
    "        n_jobs=-1,\n",
    "        tree_method= \"gpu_hist\",\n",
    "        device_type=\"cuda\"\n",
    "    )\n",
    "    lgbm_meta_params = dict(\n",
    "        n_estimators=hp['lgbm_meta_estimators'],\n",
    "        max_depth=hp['lgbm_meta_max_depth'],\n",
    "        learning_rate=hp['lgbm_meta_learning_rate'],\n",
    "        reg_lambda=hp['lgbm_meta_reg_lambda'],\n",
    "        early_stopping_round=hp['lgbm_meta_early_stopping'],\n",
    "        metric='binary_logloss',\n",
    "        tree_learner=\"serial\",\n",
    "        device=\"cpu\",\n",
    "        verbosity=-1,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    base_meta_models = [\n",
    "        ('catboost', CatWithEval(\n",
    "            **cat_meta_params,\n",
    "            eval_set=[(X_val_meta, y_val_meta)],\n",
    "            callbacks=[CatBoostPruningCallback(trial, \"F1\")])),\n",
    "        ('xgboost', XGBWithEval(\n",
    "            **xgb_meta_params, \n",
    "            eval_set=[(X_val_meta, y_val_meta)],\n",
    "            callbacks=[XGBoostPruningCallback(trial, \"validation_0-auc\")])),\n",
    "        ('lightgbm', LGBMWithEval(\n",
    "            **lgbm_meta_params, \n",
    "            eval_set=[(X_val_meta, y_val_meta)],\n",
    "            callbacks=[LightGBMPruningCallback(trial, \"binary_logloss\")])),\n",
    "    ]\n",
    "\n",
    "    model_meta = VotingClassifier(\n",
    "            estimators=base_meta_models,\n",
    "            voting='soft',\n",
    "            flatten_transform=False,\n",
    "            n_jobs=1\n",
    "        )\n",
    "    # print(\"training meta model...\")\n",
    "    # start_time = time.time()\n",
    "    model_meta.fit(X_train_meta, y_train_meta)\n",
    "    # print(f\"meta model trained in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # ── evaluación ───────────────────────────────────────────────\n",
    "    # print(\"evaluating in-sample...\")\n",
    "    # start_time = time.time()\n",
    "    r2_ins = robust_oos_score_one_direction(\n",
    "        ds_train,\n",
    "        [model_main, model_meta],\n",
    "        direction = hp['direction'],\n",
    "        n_sim = 100, mc_mode=\"both\", agg=\"q05\"\n",
    "    )\n",
    "    # print(f\"in-sample score calculated in {time.time() - start_time:.2f} seconds\")\n",
    "    # print(\"evaluating out-of-sample...\")\n",
    "    # start_time = time.time()\n",
    "    r2_oos = walk_forward_score_one_direction(\n",
    "        ds_test,\n",
    "        [model_main, model_meta],\n",
    "        direction     = hp['direction'],\n",
    "        n_sim = 100, mc_mode=\"both\",\n",
    "        agg       = \"q05\",\n",
    "        final_agg = \"median\"\n",
    "    )\n",
    "    # print(f\"out-of-sample score calculated in {time.time() - start_time:.2f} seconds /n\")\n",
    "    return r2_ins, r2_oos, model_main, model_meta\n",
    "\n",
    "def objective(trial: optuna.trial.Trial, base_hp: Dict[str, Any], study=None) -> float:\n",
    "    def calc_score(fwd: float, bwd: float, eps: float = 1e-9) -> float:\n",
    "        if (fwd is None or bwd is None or\n",
    "            not np.isfinite(fwd) or not np.isfinite(bwd) or\n",
    "            fwd <= 0 or bwd <= 0):\n",
    "            return -1.0\n",
    "        mean = 0.4 * fwd + 0.6 * bwd\n",
    "        if fwd < bwd * 0.8:\n",
    "            mean *= 0.8\n",
    "        delta  = abs(fwd - bwd) / max(abs(fwd), abs(bwd), eps)\n",
    "        score  = mean * (1.0 - delta)\n",
    "        return score\n",
    "    \n",
    "    def sample_cat_params(trial, prefix: str):\n",
    "        iterations = trial.suggest_int(f\"{prefix}_iterations\", 100, 300, step=50)\n",
    "        depth      = trial.suggest_int(f\"{prefix}_depth\", 3, 6)\n",
    "        lr         = trial.suggest_float(f\"{prefix}_learning_rate\", 0.15, 0.3, log=True)\n",
    "        l2         = trial.suggest_float(f\"{prefix}_l2_leaf_reg\", 1.0, 5.0, log=True)\n",
    "        es_rounds  = trial.suggest_int(f\"{prefix}_early_stopping\", 30, 60, step=10)\n",
    "        return {\n",
    "            f\"{prefix}_iterations\": iterations,\n",
    "            f\"{prefix}_depth\": depth,\n",
    "            f\"{prefix}_learning_rate\": lr,\n",
    "            f\"{prefix}_l2_leaf_reg\": l2,\n",
    "            f\"{prefix}_early_stopping\": es_rounds,\n",
    "        }\n",
    "    \n",
    "    def sample_xgb_params(trial, prefix: str):\n",
    "        n_estimators = trial.suggest_int(f\"{prefix}_estimators\", 100, 300, step=50)\n",
    "        max_depth = trial.suggest_int(f\"{prefix}_max_depth\", 3, 6)\n",
    "        lr           = trial.suggest_float(f\"{prefix}_learning_rate\", 0.15, 0.3, log=True)\n",
    "        reg_lambda   = trial.suggest_float(f\"{prefix}_reg_lambda\", 1.0, 5.0, log=True)\n",
    "        es_rounds    = trial.suggest_int(f\"{prefix}_early_stopping\", 30, 60, step=10)\n",
    "        return {\n",
    "            f\"{prefix}_estimators\": n_estimators,\n",
    "            f\"{prefix}_max_depth\": max_depth,\n",
    "            f\"{prefix}_learning_rate\": lr,\n",
    "            f\"{prefix}_reg_lambda\": reg_lambda,\n",
    "            f\"{prefix}_early_stopping\": es_rounds,\n",
    "        }\n",
    "\n",
    "    def sample_lgbm_params(trial, prefix: str):\n",
    "        n_estimators = trial.suggest_int(f\"{prefix}_estimators\", 100, 300, step=50)\n",
    "        max_depth    = trial.suggest_int(f\"{prefix}_max_depth\", 3, 6)\n",
    "        lr           = trial.suggest_float(f\"{prefix}_learning_rate\", 0.15, 0.3, log=True)\n",
    "        reg_lambda   = trial.suggest_float(f\"{prefix}_reg_lambda\", 1.0, 5.0, log=True)\n",
    "        es_rounds    = trial.suggest_int(f\"{prefix}_early_stopping\", 30, 60, step=10)\n",
    "        return {\n",
    "            f\"{prefix}_estimators\": n_estimators,\n",
    "            f\"{prefix}_max_depth\": max_depth,\n",
    "            f\"{prefix}_learning_rate\": lr,\n",
    "            f\"{prefix}_reg_lambda\": reg_lambda,\n",
    "            f\"{prefix}_early_stopping\": es_rounds,\n",
    "        }\n",
    "\n",
    "    gc.collect()\n",
    "    best_combined_score = -math.inf\n",
    "    hp = {k: copy.deepcopy(v) for k, v in base_hp.items() if k != 'base_df'}\n",
    "\n",
    "    # µ··· Espacio de búsqueda optimizado ···µ\n",
    "    hp['n_clusters'] = trial.suggest_int('n_clusters', 5, 50, step=5)\n",
    "    hp['k'] = trial.suggest_int('k', 2, 10, step=1)\n",
    "    hp['atr_period'] = trial.suggest_int('atr_period', 5, 50, step=5)\n",
    "    hp['markup'] = trial.suggest_float(\"markup\", 0.1, 1.0, step=0.1)\n",
    "    hp['label_max'] = trial.suggest_int('label_max', 2, 6, step=1, log=True)\n",
    "    #hp['step'] = trial.suggest_int('step', 1, hp['label_max'])\n",
    "\n",
    "    # Optimización de hiperparámetros\n",
    "    hp.update(sample_cat_params(trial, \"cat_main\"))\n",
    "    hp.update(sample_cat_params(trial, \"cat_meta\"))\n",
    "    hp.update(sample_xgb_params(trial, \"xgb_main\"))\n",
    "    hp.update(sample_xgb_params(trial, \"xgb_meta\"))\n",
    "    hp.update(sample_lgbm_params(trial, \"lgbm_main\"))\n",
    "    hp.update(sample_lgbm_params(trial, \"lgbm_meta\"))\n",
    "\n",
    "    # Optimización de períodos para el modelo principal\n",
    "    n_periods_main = trial.suggest_int('n_periods_main', 5, 15, log=True)\n",
    "    main_periods = []\n",
    "    for i in range(n_periods_main):\n",
    "        period_main = trial.suggest_int(f'period_main_{i}', 2, 200, log=True)\n",
    "        main_periods.append(period_main)\n",
    "    main_periods = sorted(list(set(main_periods)))\n",
    "    hp['periods_main'] = main_periods\n",
    "    # Selección de estadísticas para el modelo principal\n",
    "    main_stat_choices = [\n",
    "        \"std\", \"skew\", \"kurt\", \"zscore\", \"range\", \"mad\", \"entropy\", \n",
    "        \"slope\", \"momentum\", \"fractal\", \"hurst\", \"autocorr\", \"max_dd\", \n",
    "        \"sharpe\", \"fisher\", \"chande\", \"var\", \"approx_entropy\", \n",
    "        \"eff_ratio\", \"corr_skew\", \"jump_vol\", \"vol_skew\", \"hurst\"\n",
    "    ]\n",
    "    n_main_stats = trial.suggest_int('n_main_stats', 1, 5, log=True)\n",
    "    selected_main_stats = []\n",
    "    for i in range(n_main_stats):\n",
    "        stat = trial.suggest_categorical(f'main_stat_{i}', main_stat_choices)\n",
    "        selected_main_stats.append(stat)\n",
    "    selected_main_stats = list(set(selected_main_stats))\n",
    "    hp[\"stats_main\"] = selected_main_stats\n",
    "    #print(f\"Main features seleccionadas: {hp['stats_main']}\")\n",
    "\n",
    "    # Optimización de períodos para el meta-modelo\n",
    "    n_periods_meta = 1 # trial.suggest_int('n_periods_meta', 1, 3, log=True)\n",
    "    meta_periods = []\n",
    "    for i in range(n_periods_meta):\n",
    "        period_meta = trial.suggest_int(f'period_meta_{i}', 2, 6, log=True)\n",
    "        meta_periods.append(period_meta)\n",
    "    meta_periods = sorted(list(set(meta_periods)))\n",
    "    hp['periods_meta'] = meta_periods\n",
    "    # Selección de estadísticas para el meta-modelo\n",
    "    meta_stat_choices = [\n",
    "        \"std\", \"skew\", \"kurt\", \"zscore\", \"range\", \"mad\", \"entropy\", \n",
    "        \"slope\", \"momentum\", \"fractal\", \"hurst\", \"autocorr\", \"max_dd\", \n",
    "        \"sharpe\", \"fisher\", \"chande\", \"var\", \"approx_entropy\", \n",
    "        \"eff_ratio\", \"corr_skew\", \"jump_vol\", \"vol_skew\", \"hurst\"\n",
    "    ]\n",
    "    n_meta_stats = trial.suggest_int('n_meta_stats', 1, 3, log=True)\n",
    "    selected_meta_stats = []\n",
    "    for i in range(n_meta_stats):\n",
    "        stat = trial.suggest_categorical(f'meta_stat_{i}', meta_stat_choices)\n",
    "        selected_meta_stats.append(stat)\n",
    "    selected_meta_stats = list(set(selected_meta_stats))\n",
    "    hp[\"stats_meta\"] = selected_meta_stats\n",
    "    #print(f\"Meta features seleccionadas: {hp['stats_meta']} | Periodo: {hp['periods_meta']}\")\n",
    "\n",
    "    # Dataset completo obtener caracteristicas\n",
    "    full_ds = get_features(base_hp['base_df'], hp)\n",
    "    # Seccionar dataset de entrenamiento\n",
    "    test_mask  = (full_ds.index >= hp[\"test_start\"]) & (full_ds.index <= hp[\"test_end\"])\n",
    "    train_mask = (full_ds.index >= hp[\"train_start\"]) & (full_ds.index <= hp[\"train_end\"]) & ~test_mask\n",
    "    ds_train = full_ds[train_mask]\n",
    "    ds_test  = full_ds[test_mask]\n",
    "\n",
    "    # Clustering\n",
    "    ds_train = sliding_window_clustering(\n",
    "        ds_train,\n",
    "        n_clusters=hp['n_clusters'],\n",
    "        step=hp.get('step', None),\n",
    "        atr_period=hp['atr_period'],\n",
    "        k=hp['k']\n",
    "    )\n",
    "    # Evaluar clusters ordenados por tamaño\n",
    "    cluster_sizes = ds_train['clusters'].value_counts()\n",
    "    # Filtrar el cluster 0 (inválido) si existe\n",
    "    if 0 in cluster_sizes.index:\n",
    "        cluster_sizes = cluster_sizes.drop(0)\n",
    "    for clust in cluster_sizes.index:\n",
    "        # Main data\n",
    "        main_data = ds_train.loc[ds_train['clusters'] == clust]\n",
    "        if len(main_data) <= hp['label_max']:\n",
    "            continue\n",
    "        main_data = get_labels_one_direction(\n",
    "            main_data,\n",
    "            markup=hp['markup'],\n",
    "            max_val=hp['label_max'],\n",
    "            direction=hp['direction'],\n",
    "            atr_period=hp['atr_period'],\n",
    "            deterministic=True\n",
    "        )\n",
    "        if (main_data['labels'].value_counts() < 2).any():\n",
    "            continue\n",
    "\n",
    "        # Meta data\n",
    "        meta_data = ds_train.copy()\n",
    "        meta_data['clusters'] = (meta_data['clusters'] == clust).astype(int)\n",
    "        if (meta_data['clusters'].value_counts() < 2).any():\n",
    "            continue\n",
    "\n",
    "        # Evaluación en ambos períodos\n",
    "        r2_ins, r2_oos, model_main, meta_model = fit_final_models(\n",
    "            main_data,\n",
    "            meta_data,\n",
    "            ds_train,\n",
    "            ds_test,\n",
    "            hp,\n",
    "            trial\n",
    "        )\n",
    "        if r2_ins == None or r2_oos == None or model_main == None or meta_model == None:\n",
    "            continue\n",
    "        # Calcular puntuación combinada (puedes ajustar los pesos según necesites)\n",
    "        score = calc_score(r2_ins, r2_oos)\n",
    "        if score <= -1.0:\n",
    "             continue\n",
    "\n",
    "        if score > best_combined_score:\n",
    "            best_combined_score = score\n",
    "            # Guardar información del trial actual\n",
    "            trial.set_user_attr(\"r2_ins\", r2_ins)\n",
    "            trial.set_user_attr(\"r2_oos\", r2_oos)\n",
    "            trial.set_user_attr(\"combined_score\", score)\n",
    "            trial.set_user_attr(\"cluster_id\", clust)\n",
    "            trial.set_user_attr(\"stats_main\", hp[\"stats_main\"])\n",
    "            trial.set_user_attr(\"stats_meta\", hp[\"stats_meta\"])\n",
    "            trial.set_user_attr(\"periods_main\", hp[\"periods_main\"])\n",
    "            trial.set_user_attr(\"periods_meta\", hp[\"periods_meta\"])\n",
    "            # Guardar parámetros del trial actual (sin fechas)\n",
    "            params_to_save = hp.copy()\n",
    "            params_to_save.pop('backward', None)\n",
    "            params_to_save.pop('forward', None)\n",
    "            params_to_save.pop('full_forward', None)\n",
    "            trial.set_user_attr(\"params\", params_to_save)\n",
    "            # Si existe el estudio, actualizar sus atributos\n",
    "            if study is not None:\n",
    "                current_best = study.user_attrs.get(\"best_combined_score\", -math.inf)\n",
    "                if score > current_best:\n",
    "                    study.set_user_attr(\"best_params\", params_to_save)\n",
    "                    study.set_user_attr(\"best_metrics\", {\n",
    "                        \"r2_ins\": r2_ins,\n",
    "                        \"r2_oos\": r2_oos,\n",
    "                        \"combined_score\": score,\n",
    "                        \"cluster_id\": clust,\n",
    "                    })\n",
    "                    study.set_user_attr(\"best_combined_score\", score)\n",
    "                    study.set_user_attr(\"best_models\", [model_main, meta_model])\n",
    "                    study.set_user_attr(\"best_stats_main\", hp[\"stats_main\"])\n",
    "                    study.set_user_attr(\"best_stats_meta\", hp[\"stats_meta\"])\n",
    "                    study.set_user_attr(\"best_periods_main\", hp[\"periods_main\"])\n",
    "                    study.set_user_attr(\"best_periods_meta\", hp[\"periods_meta\"])\n",
    "                    study.set_user_attr(\"best_trial_number\", trial.number)\n",
    "                    study.set_user_attr(\"best_df_sample\", full_ds.sample(1))\n",
    "    # Si no hay ningún cluster válido, devolver un valor negativo pero no infinito\n",
    "    if best_combined_score == -math.inf:\n",
    "        return -1.0\n",
    "\n",
    "    # No aplicar penalización adicional por pocos clusters para mantener coherencia\n",
    "    return best_combined_score\n",
    "\n",
    "def optimize_and_export(base_hp: Dict[str, Any]):\n",
    "    \"\"\"Lanza Optuna, guarda el mejor modelo y lo exporta a ONNX.\"\"\"\n",
    "    def show_best_summary(study: optuna.study.Study, model_seed) -> None:\n",
    "        \"\"\"Muestra en un único cuadro el resultado óptimo del estudio.\"\"\"\n",
    "        # ── extraer métricas y modelos ───────────────────────────────────────\n",
    "        m  = study.user_attrs.get(\"best_metrics\", {})\n",
    "        f2 = m.get(\"r2_ins\",   float(\"nan\"))\n",
    "        b2 = m.get(\"r2_oos\",  float(\"nan\"))\n",
    "        c2 = m.get(\"combined_score\", float(\"nan\"))\n",
    "        best_trial = study.user_attrs.get(\"best_trial_number\", \"-\")\n",
    "        best_df_sample = study.user_attrs.get(\"best_df_sample\", None)\n",
    "\n",
    "        main_cls = meta_cls = \"None\"\n",
    "        best_models = study.user_attrs.get(\"best_models\")\n",
    "        if best_models and len(best_models) == 2:\n",
    "            main_cls = type(best_models[0]).__name__\n",
    "            meta_cls = type(best_models[1]).__name__\n",
    "\n",
    "        # ── cuadro de resumen ────────────────────────────────────────────────\n",
    "        lines = [\n",
    "            \"┌\" + \"─\" * 55 + \"┐\",\n",
    "            f\"│  MODELO {model_seed} TRIAL ÓPTIMO #{best_trial}\",\n",
    "            \"├\" + \"─\" * 55 + \"┤\",\n",
    "            f\"│  R² INS : {f2:10.4f}  │  R² OOS : {b2:10.4f} │\",\n",
    "            f\"│  Combined     : {c2:10.4f}                            │\",\n",
    "            \"├\" + \"─\" * 55 + \"┤\"\n",
    "        ]\n",
    "        print(\"\\n\".join(lines))\n",
    "\n",
    "    base_hp.update({\n",
    "        'model_seed': random.randint(0, 10000000),\n",
    "    })\n",
    "    # Configurar el pruner inteligente\n",
    "    # pruner = SuccessiveHalvingPruner(\n",
    "    #     min_resource=1,\n",
    "    #     reduction_factor=3,\n",
    "    #     min_early_stopping_rate=0\n",
    "    # )\n",
    "\n",
    "    # Crear el estudio sin persistencia\n",
    "    study = optuna.create_study(\n",
    "        direction='maximize',\n",
    "        pruner=HyperbandPruner(),\n",
    "        sampler=optuna.samplers.TPESampler(\n",
    "            n_startup_trials=int(np.sqrt(base_hp['n_trials'])),\n",
    "        )\n",
    "    )\n",
    "    study.optimize(lambda t: objective(t, base_hp, study),\n",
    "                n_trials=base_hp['n_trials'],\n",
    "                show_progress_bar=True)\n",
    "\n",
    "    # ── verificación y cuadro ───────────────────────────────\n",
    "    best_trial = study.best_trial\n",
    "    assert study.user_attrs.get(\"best_trial_number\") == best_trial.number, \\\n",
    "        \"best_trial_number no coincide con study.best_trial.number\"\n",
    "\n",
    "    best_metric_saved = study.user_attrs.get(\"best_metrics\", {}).get(\"combined_score\")\n",
    "    if best_metric_saved is not None:\n",
    "        assert math.isclose(best_metric_saved, study.best_value, rel_tol=1e-9), \\\n",
    "            \"combined_score guardado ≠ study.best_value\"\n",
    "    else:\n",
    "        print(\"⚠️  Ningún trial produjo un score válido; modelos no guardados.\")\n",
    "\n",
    "    best_models = study.user_attrs.get(\"best_models\")\n",
    "    assert best_models and len(best_models) == 2 and all(best_models), \\\n",
    "        \"best_models incorrecto\"\n",
    "\n",
    "    show_best_summary(study, base_hp['model_seed'])\n",
    "\n",
    "    # ── exportación ─────────────────────────────────────────\n",
    "    export_params = base_hp.copy()\n",
    "    export_params.update({\n",
    "        \"best_trial\": study.user_attrs[\"best_trial_number\"],\n",
    "        \"best_score\": study.user_attrs[\"best_combined_score\"],\n",
    "        \"best_periods_main\": study.user_attrs[\"best_periods_main\"],\n",
    "        \"best_periods_meta\": study.user_attrs[\"best_periods_meta\"],\n",
    "        \"best_stats_main\"  : study.user_attrs[\"best_stats_main\"],\n",
    "        \"best_stats_meta\"  : study.user_attrs[\"best_stats_meta\"],\n",
    "        \"best_models\"      : study.user_attrs[\"best_models\"],\n",
    "    })\n",
    "    export_model_to_ONNX(**export_params)\n",
    "    # ── devolver métricas ─────────────────────────────────────────\n",
    "    return {\n",
    "        \"r2_ins\"   : study.user_attrs.get(\"best_metrics\", {}).get(\"r2_ins\",   float(\"nan\")),\n",
    "        \"r2_oos\"  : study.user_attrs.get(\"best_metrics\", {}).get(\"r2_oos\",  float(\"nan\")),\n",
    "        \"combined_score\": study.user_attrs.get(\"best_metrics\", {}).get(\"combined_score\", float(\"nan\")),\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_hp: Dict[str, Any] = {\n",
    "        'symbol': r'XAUUSD',\n",
    "        'timeframe': 'H1',\n",
    "        'direction': 'buy',\n",
    "        'train_start': datetime(2019, 1, 1),\n",
    "        'train_end': datetime(2025, 1, 1),\n",
    "        'test_start': datetime(2022, 1, 1),\n",
    "        'test_end': datetime(2023, 1, 1),\n",
    "        'model_seed': 0,\n",
    "        'n_trials': 1000,\n",
    "        'models_export_path': r'/mnt/c/Users/Administrador/AppData/Roaming/MetaQuotes/Terminal/6C3C6A11D1C3791DD4DBF45421BF8028/MQL5/Files/',\n",
    "        'include_export_path': r'/mnt/c/Users/Administrador/AppData/Roaming/MetaQuotes/Terminal/6C3C6A11D1C3791DD4DBF45421BF8028/MQL5/Include/ajmtrz/include/Dmitrievsky',\n",
    "        'history_path': r\"/mnt/c/Users/Administrador/AppData/Roaming/MetaQuotes/Terminal/Common/Files/\",\n",
    "        'stats_main': [],\n",
    "        'stats_meta': [],\n",
    "        'best_models': [None, None],\n",
    "        'markup': 0.20,\n",
    "        'label_min'  : 1,\n",
    "        'label_max'  : 15,\n",
    "        'n_clusters': 30,\n",
    "        'window_size': 350,\n",
    "        'periods_main': [i for i in range(5, 300, 30)],\n",
    "        'periods_meta': [5],\n",
    "    }\n",
    "    base_df = get_prices(base_hp)\n",
    "    base_hp.update({\n",
    "        'base_df': base_df,\n",
    "    })\n",
    "    # Para recopilar resultados globales de todos los modelos\n",
    "    all_results = {}\n",
    "    best_models = []\n",
    "    model_range = range(0, 5)\n",
    "    base_hp.update({'n_trials': 500})\n",
    "    for i in tqdm(model_range, desc=f\"Optimizando {base_hp['symbol']}/{base_hp['timeframe']}\", unit=\"modelo\"):\n",
    "        try:\n",
    "            model_results = optimize_and_export(base_hp)\n",
    "            best_models.append((i, model_results))\n",
    "            \n",
    "            # Añadir a resultados globales\n",
    "            all_results[f\"model_{i}\"] = {\n",
    "                \"success\": True,\n",
    "                \"r2_ins\": model_results[\"r2_ins\"],\n",
    "                \"r2_oos\": model_results[\"r2_oos\"],\n",
    "                \"combined_score\": model_results[\"combined_score\"]\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            tqdm.write(f\"\\nError procesando modelo {i}: {str(e)}\")\n",
    "            tqdm.write(traceback.format_exc())\n",
    "            \n",
    "            all_results[f\"model_{i}\"] = {\n",
    "                \"success\": False,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "            continue\n",
    "    \n",
    "    # Resumen final\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"RESUMEN DE OPTIMIZACIÓN {base_hp['symbol']}/{base_hp['timeframe']}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    successful_models = [info for model_key, info  in all_results.items() if info.get(\"success\", False)]\n",
    "    print(f\"Modelos completados exitosamente: {len(successful_models)}/{len(model_range)}\")\n",
    "    \n",
    "    if successful_models:\n",
    "        # Calcular estadísticas globales\n",
    "        r2_ins_scores = [info[\"r2_ins\"] for info in successful_models]\n",
    "        r2_oos_scores = [info[\"r2_oos\"] for info in successful_models]\n",
    "        combined_scores = [info[\"combined_score\"] for info in successful_models]\n",
    "        \n",
    "        print(f\"\\nEstadísticas de rendimiento:\")\n",
    "        print(f\"  R2 INS promedio: {np.mean(r2_ins_scores):.4f} ± {np.std(r2_ins_scores):.4f}\")\n",
    "        print(f\"  R2 OOS promedio: {np.mean(r2_oos_scores):.4f} ± {np.std(r2_oos_scores):.4f}\")\n",
    "        print(f\"  Puntuación combinada promedio: {np.mean(combined_scores):.4f} ± {np.std(combined_scores):.4f}\")\n",
    "\n",
    "        # Identificar el mejor modelo global basado en la puntuación combinada\n",
    "        successful = [(k, v) for k, v in all_results.items() if v.get(\"success\", False)]\n",
    "        combined_scores = [v[\"combined_score\"] for _, v in successful]\n",
    "        best_model_key, best_info = successful[int(np.argmax(combined_scores))]\n",
    "        \n",
    "        print(f\"\\nMejor modelo global: {best_model_key}\")\n",
    "        print(f\"  R2 INS: {best_info['r2_ins']:.4f}\")\n",
    "        print(f\"  R2 OOS: {best_info['r2_oos']:.4f}\")\n",
    "        print(f\"  Puntuación combinada: {best_info['combined_score']:.4f}\")\n",
    "    \n",
    "    print(\"\\nProceso de optimización completado.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
